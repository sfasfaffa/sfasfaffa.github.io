<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>



                  <section class="hero">
                    <div class="hero-body">
                      <div class="container is-max-desktop">
                        <div class="columns is-centered">
                          <div class="column has-text-centered">
                            <h1 class="title is-1 publication-title">Towards Reasoning Era: A Survey of Long Chain-of-Thought</h1>
                            <div class="is-size-5 publication-authors">
                              <!-- Paper authors -->
                              <span class="author-block">
                                <a href="https://lightchen233.github.io/" target="_blank">Qiguang Chen</a><sup>*</sup>,</span>
                                <span class="author-block">
                                  <a href="https://faculty.csu.edu.cn/qinlibo/zh_CN/index.htm" target="_blank">Libo Qin</a>,</span>
                                  <span class="author-block">
                                    <a href="https://github.com/FarzoneILIN" target="_blank">Jinhao Liu</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://github.com/sfasfaffa" target="_blank">Dengyun Peng</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://github.com/GoatCsu" target="_blank">Te Gao</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://aaron617.github.io/" target="_blank">MengKang Hu</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://chewanxiang.com/" target="_blank">Wanxiang Che</a>
                                  </span>
                                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Harbin Institude of Technology</span>
                    <!-- <span class="author-block">Harbin Institude of Technology<br>Conferance name and year</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in logical reasoning tasks are often attributed to test-time scaling, with many researchers suggesting that increasing model capacity to process longer reasoning sequences improves performance. However, this idea is challenged in simpler tasks, such as commonsense reasoning and basic mathematics, where test-time scaling may lead to “overthinking” that hampers model performance. This paradox remains underexplored, with existing research limited by two main shortcomings: a failure to distinguish between Long Chains of Thought (Long CoT) and Short Chains of Thought (Short CoT), and the absence of a comprehensive review on the topic. To address these issues, this survey first distinguishes between Long CoT and Short CoT, introducing a new taxonomy to categorize these reasoning paradigms. We examine the key characteristics of Long CoT—Deep Reasoning, Extensive Exploration, and Feasible Reflection—and highlight how these features enable deeper and more efficient reasoning compared to the shallow, redundancy-prone Short CoT. Our review synthesizes the current state of Long CoT research, identifies critical gaps, and suggests future research directions. We also address challenges in Long CoT, such as multi-modal reasoning, efficiency, and knowledge integration, and recommend resources, including open-source software, corpora, and key publications, to support further studies. Through this survey, we aim to offer a unified perspective on Long CoT, propose strategies to overcome existing limitations, and inspire future research to push the boundaries of logical reasoning in artificial intelligence.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="section">
  <div class="container">
    <div class="has-text-centered">
      <h1 class="title is-2">Paper List</h1>
      <p class="subtitle is-4"></p>
    </div>
    <div class="content">
      <div id="type" style="display:none;">Paper List</div>
      <ul>
        <div id="mllm-paperlist">
          <!-- Deep Reasoning -->
          <h2>Deep Reasoning</h2>

          <!-- Deep Reasoning Execution -->
          <h3>Deep Reasoning Execution</h3>
          <ul>
            <li><i><b>Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models</b></i>, Xu et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.11736" target="_blank">Link</a>]</li>
            <li><i><b>SKIntern: Internalizing Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models</b></i>, Liao et al., <code>2025.01</code></li>
            <li><i><b>Guiding language model reasoning with planning tokens</b></i>, Wang et al., <code>2023.10</code> [<a href="https://arxiv.org/abs/2310.05707" target="_blank">Link</a>]</li>
            <li><i><b>Efficient Reasoning with Hidden Thinking</b></i>, Shen et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.19201" target="_blank">Link</a>]</li>
            <li><i><b>Scalable Language Models with Posterior Inference of Latent Thought Vectors</b></i>, Kong et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01567" target="_blank">Link</a>]</li>
            <li><i><b>Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13842" target="_blank">Link</a>]</li>
            <li><i><b>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</b></i>, Geiping et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05171" target="_blank">Link</a>]</li>
            <li><i><b>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</b></i>, Wei et al., <code>2022.11</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf" target="_blank">Link</a>]</li>
            <li><i><b>Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages</b></i>, Qin et al., <code>2023.07</code></li>
            <li><i><b>{A}uto{CAP}: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought</b></i>, Zhang et al., <code>2024.08</code> [<a href="https://aclanthology.org/2024.findings-acl.546/" target="_blank">Link</a>]</li>
            <li><i><b>CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07316" target="_blank">Link</a>]</li>
            <li><i><b>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</b></i>, Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen et al., <code>2023.11</code> [<a href="https://openreview.net/forum?id=YfZ4ZPt8zd" target="_blank">Link</a>]</li>
            <li><i><b>Chain of Code: Reasoning with a Language Model-Augmented Code Emulator</b></i>, Li et al., <code>2024.07</code> [<a href="https://proceedings.mlr.press/v235/li24ar.html" target="_blank">Link</a>]</li>
            <li><i><b>Lean-star: Learning to interleave thinking and proving</b></i>, Lin et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.10040" target="_blank">Link</a>]</li>
            <li><i><b>Quiet-star: Language models can teach themselves to think before speaking</b></i>, Zelikman et al., <code>2024.03</code> [<a href="https://arxiv.org/abs/2403.09629" target="_blank">Link</a>]</li>
            <li><i><b>Training large language models to reason in a continuous latent space</b></i>, Hao et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.06769" target="_blank">Link</a>]</li>
          </ul>

          <!-- Deep Reasoning Learning -->
          <h3>Deep Reasoning Learning</h3>
          <ul>
            <li><i><b>Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models</b></i>, Chijiwa et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12776" target="_blank">Link</a>]</li>
            <li><i><b>STeCa: Step-level Trajectory Calibration for LLM Agent Learning</b></i>, Wang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14276" target="_blank">Link</a>]</li>
            <li><i><b>V-star: Training verifiers for self-taught reasoners</b></i>, Hosseini et al., <code>2024.02</code> [<a href="https://arxiv.org/abs/2402.06457" target="_blank">Link</a>]</li>
            <li><i><b>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</b></i>, Singh et al., <code>2024.04</code></li>
            <li><i><b>FastMCTS: A Simple Sampling Strategy for Data Synthesis</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11476" target="_blank">Link</a>]</li>
            <li><i><b>Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models</b></i>, Xu et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.11736" target="_blank">Link</a>]</li>
            <li><i><b>LLMs Can Teach Themselves to Better Predict the Future</b></i>, Turtel et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05253" target="_blank">Link</a>]</li>
            <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
            <li><i><b>Sft memorizes, rl generalizes: A comparative study of foundation model post-training</b></i>, Chu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17161" target="_blank">Link</a>]</li>
            <li><i><b>Demystifying Long Chain-of-Thought Reasoning in LLMs</b></i>, Yeo et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03373" target="_blank">Link</a>]</li>
            <li><i><b>Advancing Math Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages</b></i>, Chen et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.14002" target="_blank">Link</a>]</li>
            <li><i><b>Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls</b></i>, Wang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11183" target="_blank">Link</a>]</li>
            <li><i><b>Large Language Models Are Reasoning Teachers</b></i>, Ho et al., <code>2023.07</code> [<a href="https://aclanthology.org/2023.acl-long.830/" target="_blank">Link</a>]</li>
            <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
            <li><i><b>Sft memorizes, rl generalizes: A comparative study of foundation model post-training</b></i>, Chu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17161" target="_blank">Link</a>]</li>
            <li><i><b>Open R1</b></i>, Huggingface Team et al., <code>2025.01</code></li>
            <li><i><b>Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus</b></i>, Morishita et al., <code>2024.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/8678da90126aa58326b2fc0254b33a8c-Paper-Conference.pdf" target="_blank">Link</a>]</li>
            <li><i><b>System-2 Mathematical Reasoning via Enriched Instruction Tuning</b></i>, Cai et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16964" target="_blank">Link</a>]</li>
            <li><i><b>Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes</b></i>, Chen et al., <code>2024.03</code> [<a href="https://arxiv.org/abs/2403.00800" target="_blank">Link</a>]</li>
            <li><i><b>Distillation Scaling Laws</b></i>, Busbridge et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.08606" target="_blank">Link</a>]</li>
            <li><i><b>Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization</b></i>, Yao et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.04667" target="_blank">Link</a>]</li>
            <li><i><b>O1 Replication Journey--Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?</b></i>, Huang et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.16489" target="_blank">Link</a>]</li>
            <li><i><b>Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems</b></i>, Min et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.09413" target="_blank">Link</a>]</li>
            <li><i><b>Openai o1 system card</b></i>, Jaech et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16720" target="_blank">Link</a>]</li>
            <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
            <li><i><b>LIMO: Less is More for Reasoning</b></i>, Ye et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03387" target="_blank">Link</a>]</li>
            <li><i><b>RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?</b></i>, Xu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.11284" target="_blank">Link</a>]</li>
            <li><i><b>Star: Bootstrapping reasoning with reasoning</b></i>, Zelikman et al., <code>2022.11</code></li>
            <li><i><b>BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation</b></i>, Pang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03860" target="_blank">Link</a>]</li>
            <li><i><b>Reinforced self-training (rest) for language modeling</b></i>, Gulcehre et al., <code>2023.08</code> [<a href="https://arxiv.org/abs/2308.08998" target="_blank">Link</a>]</li>
            <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
        <h2>Extensive Exploration</h2>
        <h3>Exploration Scaling</h3>
          <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
          <li><i><b>Complexity-Based Prompting for Multi-step Reasoning</b></i>, Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot et al., <code>2023.02</code> [<a href="https://openreview.net/forum?id=yf1icZHC-l9" target="_blank">Link</a>]</li>
          <li><i><b>Openai o1 system card</b></i>, Jaech et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16720" target="_blank">Link</a>]</li>
          <li><i><b>s1: Simple test-time scaling</b></i>, Muennighoff et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.19393" target="_blank">Link</a>]</li>
          <li><i><b>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</b></i>, Geiping et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05171" target="_blank">Link</a>]</li>
          <li><i><b>The Impact of Reasoning Step Length on Large Language Models</b></i>, Jin et al., <code>2024.08</code> [<a href="https://aclanthology.org/2024.findings-acl.108/" target="_blank">Link</a>]</li>
          <li><i><b>When More is Less: Understanding Chain-of-Thought Length in LLMs</b></i>, Wu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07266" target="_blank">Link</a>]</li>
          <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
          <li><i><b>Self-Consistency Improves Chain of Thought Reasoning in Language Models</b></i>, Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou et al., <code>2023.02</code> [<a href="https://openreview.net/forum?id=1PL1NIMMrw" target="_blank">Link</a>]</li>
          <li><i><b>Confidence Improves Self-Consistency in LLMs</b></i>, Taubenfeld et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06233" target="_blank">Link</a>]</li>
          <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
          <li><i><b>Lachesis: Predicting LLM Inference Accuracy using Structural Properties of Reasoning Paths</b></i>, Kim et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.08281" target="_blank">Link</a>]</li>
          <li><i><b>From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning</b></i>, Li et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.11877" target="_blank">Link</a>]</li>
          <li><i><b>Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models</b></i>, Wu et al., <code>2024.08</code> [<a href="https://arxiv.org/abs/2408.00724" target="_blank">Link</a>]</li>
          <li><i><b>Improve Mathematical Reasoning in Language Models by Automated Process Supervision</b></i>, Luo et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.06592" target="_blank">Link</a>]</li>
          <li><i><b>Seed-cts: Unleashing the power of tree search for superior performance in competitive coding tasks</b></i>, Wang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.12544" target="_blank">Link</a>]</li>
          <li><i><b>Beyond examples: High-level automated reasoning paradigm in in-context learning via mcts</b></i>, Wu et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.18478" target="_blank">Link</a>]</li>
          <li><i><b>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</b></i>, Liu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06703" target="_blank">Link</a>]</li>
          <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
          <li><i><b>Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?</b></i>, Zeng et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12215" target="_blank">Link</a>]</li>  
          <li><i><b>Optimizing Temperature for Language Models with Multi-Sample Inference</b></i>, Du et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05234" target="_blank">Link</a>]</li>
          <li><i><b>Scaling llm inference with optimized sample compute allocation</b></i>, Zhang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.22480" target="_blank">Link</a>]</li>
          <li><i><b>Bag of Tricks for Inference-time Computation of LLM Reasoning</b></i>, Liu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07191" target="_blank">Link</a>]</li>
          <li><i><b>Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages</b></i>, Qin et al., <code>2023.07</code></li>
          <li><i><b>Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective</b></i>, Yu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.11110" target="_blank">Link</a>]</li>
          <li><i><b>Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights</b></i>, Parashar et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12521" target="_blank">Link</a>]</li>
          <li><i><b>Examining False Positives under Inference Scaling for Mathematical Reasoning</b></i>, Wang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06217" target="_blank">Link</a>]</li>
          <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
          <li><i><b>From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models</b></i>, Sean Welleck and Amanda Bertsch and Matthew Finlayson and Hailey Schoelkopf and Alex Xie and Graham Neubig and Ilia Kulikov and Zaid Harchaoui et al., <code>2024.11</code> [<a href="https://openreview.net/forum?id=eskQMcIbMS" target="_blank">Link</a>]</li>
          <li><i><b>Test-time Computing: from System-1 Thinking to System-2 Thinking</b></i>, Ji et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.02497" target="_blank">Link</a>]</li>
          <li><i><b>Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07803" target="_blank">Link</a>]</li>  
          <li><i><b>PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models</b></i>, Anderson et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01584" target="_blank">Link</a>]</li>
          <li><i><b>Scaling Test-Time Compute Without Verification or RL is Suboptimal</b></i>, Setlur et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12118" target="_blank">Link</a>]</li>
          <li><i><b>Inference Scaling vs Reasoning: An Empirical Analysis of Compute-Optimal LLM Problem-Solving</b></i>, AbdElhameed et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16260" target="_blank">Link</a>]</li>        
          <li><i><b>Scaling Inference Computation: Compute-Optimal Inference for Problem-Solving with Language Models</b></i>, Yangzhen Wu and Zhiqing Sun and Shanda Li and Sean Welleck and Yiming Yang et al., <code>2024.10</code> [<a href="https://openreview.net/forum?id=j7DZWSc8qu" target="_blank">Link</a>]</li>
          <li><i><b>Don't Trust: Verify -- Grounding {LLM} Quantitative Reasoning with Autoformalization</b></i>, Jin Peng Zhou and Charles E Staats and Wenda Li and Christian Szegedy and Kilian Q Weinberger and Yuhuai Wu et al., <code>2024.01</code> [<a href="https://openreview.net/forum?id=V5tdi14ple" target="_blank">Link</a>]</li>
          <li><i><b>Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification</b></i>, Zhao et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01839" target="_blank">Link</a>]</li>
          <li><i><b>Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information</b></i>, Zhang et al., <code>2024.11</code> [<a href="https://aclanthology.org/2024.findings-emnlp.388/" target="_blank">Link</a>]</li>
          <li><i><b>Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers</b></i>, Raza et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.16961" target="_blank">Link</a>]</li>     
          <li><i><b>SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling</b></i>, Chen et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.19306" target="_blank">Link</a>]</li>
          <li><i><b>Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation</b></i>, Lyzhov et al., <code>2020.08</code> [<a href="https://proceedings.mlr.press/v124/lyzhov20a.html" target="_blank">Link</a>]</li>
          <li><i><b>Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision</b></i>, Wang et al., <code>2024.02</code> [<a href="https://arxiv.org/abs/2402.02658" target="_blank">Link</a>]</li>  
          <li><i><b>DnA-Eval: Enhancing Large Language Model Evaluation through Decomposition and Aggregation</b></i>, Li et al., <code>2025.01</code></li>
          <li><i><b>ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification</b></i>, Lee et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14565" target="_blank">Link</a>]</li>
          <li><i><b>S*: Test Time Scaling for Code Generation</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14382" target="_blank">Link</a>]</li>
          <li><i><b>MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning</b></i>, Chen et al., <code>2024.09</code> [<a href="https://arxiv.org/abs/2409.12147" target="_blank">Link</a>]</li>
          <li><i><b>Making large language models better reasoners with step-aware verifier</b></i>, Li et al., <code>2022.06</code> [<a href="https://arxiv.org/abs/2206.02336" target="_blank">Link</a>]</li>
          <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
          <li><i><b>A simple and provable scaling law for the test-time compute of large language models</b></i>, Chen et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.19477" target="_blank">Link</a>]</li>
          <li><i><b>Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs</b></i>, Wu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.10858" target="_blank">Link</a>]</li>
        <h3>Internal Exploration</h3>
          <li><i><b>Sft memorizes, rl generalizes: A comparative study of foundation model post-training</b></i>, Chu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17161" target="_blank">Link</a>]</li>
          <li><i><b>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</b></i>, Shen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.02508" target="_blank">Link</a>]</li>
          <li><i><b>Demystifying Long Chain-of-Thought Reasoning in LLMs</b></i>, Yeo et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03373" target="_blank">Link</a>]</li>
          <li><i><b>Proximal policy optimization algorithms</b></i>, Schulman et al., <code>2017.07</code> [<a href="https://arxiv.org/abs/1707.06347" target="_blank">Link</a>]</li>
          <li><i><b>REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models</b></i>, Hu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.03262" target="_blank">Link</a>]</li>
          <li><i><b>Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning</b></i>, Vassoyan et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06533" target="_blank">Link</a>]</li>
          <li><i><b>Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization</b></i>, Liu et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.18279" target="_blank">Link</a>]</li> 
          <li><i><b>LIMR: Less is More for RL Scaling</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11886" target="_blank">Link</a>]</li>
          <li><i><b>Training Language Models to Reason Efficiently</b></i>, Arora et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.04463" target="_blank">Link</a>]</li>
          <li><i><b>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</b></i>, Singh et al., <code>2024.04</code></li>
          <li><i><b>o1-coder: an o1 replication for coding</b></i>, Zhang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.00154" target="_blank">Link</a>]</li>
          <li><i><b>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</b></i>, Xie et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14768" target="_blank">Link</a>]</li>
          <li><i><b>On the Emergence of Thinking in LLMs I: Searching for the Right Intuition</b></i>, Ye et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06773" target="_blank">Link</a>]</li>
          <li><i><b>Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling</b></i>, Hou et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.11651" target="_blank">Link</a>]</li>
          <li><i><b>Process reinforcement through implicit rewards</b></i>, Cui et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01456" target="_blank">Link</a>]</li>
          <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
          <li><i><b>Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights</b></i>, Parashar et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12521" target="_blank">Link</a>]</li>
          <li><i><b>Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation</b></i>, Kim et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01694" target="_blank">Link</a>]</li>
          <li><i><b>Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges</b></i>, Shrestha et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.08680" target="_blank">Link</a>]</li>
          <li><i><b>Policy Guided Tree Search for Enhanced LLM Reasoning</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06813" target="_blank">Link</a>]</li>
          <li><i><b>RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold</b></i>, Setlur et al., <code>2024.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/4b77d5b896c321a29277524a98a50215-Paper-Conference.pdf" target="_blank">Link</a>]</li>
          <li><i><b>Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment</b></i>, Sun et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.00203" target="_blank">Link</a>]</li>
          <li><i><b>Diverse Preference Optimization</b></i>, Lanchantin et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.18101" target="_blank">Link</a>]</li>
          <li><i><b>A Small Step Towards Reproducing OpenAI o1: Progress Report on the Steiner Open Source Models</b></i>, Yichao Ji et al., <code>2024.10</code> [<a href="https://medium.com/@peakji/b9a756a00855" target="_blank">Link</a>]</li>  
          <li><i><b>COS (M+ O) S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via Language Models</b></i>, Materzok et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17104" target="_blank">Link</a>]</li>
          <li><i><b>A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications</b></i>, Xiao et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.15595" target="_blank">Link</a>]</li>   
          <li><i><b>Deepseekmath: Pushing the limits of mathematical reasoning in open language models</b></i>, Shao et al., <code>2024.02</code> [<a href="https://arxiv.org/abs/2402.03300" target="_blank">Link</a>]</li>
          <li><i><b>Offline Reinforcement Learning for LLM Multi-Step Reasoning</b></i>, Wang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16145" target="_blank">Link</a>]</li>
          <li><i><b>CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning Tasks</b></i>, Wang et al., <code>2024.09</code> [<a href="https://arxiv.org/abs/2409.08642" target="_blank">Link</a>]</li>
          <li><i><b>Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability</b></i>, Lin et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.19943" target="_blank">Link</a>]</li>
          <li><i><b>Focused-DPO: Enhancing Code Generation Through Focused Preference Optimization on Error-Prone Points</b></i>, Zhang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11475" target="_blank">Link</a>]</li>      
          <li><i><b>Reasoning with Reinforced Functional Token Tuning</b></i>, Zhang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13389" target="_blank">Link</a>]</li>
          <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
          <li><i><b>Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning</b></i>, Lyu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06781" target="_blank">Link</a>]</li>
          <li><i><b>Competitive Programming with Large Reasoning Models</b></i>, El-Kishky et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06807" target="_blank">Link</a>]</li>
          <li><i><b>Thinking fast and slow with deep learning and tree search</b></i>, Anthony et al., <code>2017.12</code></li>
          <li><i><b>AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training</b></i>, Ziyu Wan and Xidong Feng and Muning Wen and Stephen Marcus McAleer and Ying Wen and Weinan Zhang and Jun Wang et al., <code>2024.05</code> [<a href="https://openreview.net/forum?id=C4OpREezgj" target="_blank">Link</a>]</li>
          <li><i><b>Re{ST}-{MCTS}*: {LLM} Self-Training via Process Reward Guided Tree Search</b></i>, Dan Zhang and Sining Zhoubian and Ziniu Hu and Yisong Yue and Yuxiao Dong and Jie Tang et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=8rcFOqEud5" target="_blank">Link</a>]</li>
          <li><i><b>DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL</b></i>, Michael Luo and Sijun Tan and Justin Wong and Xiaoxiang Shi and William Y. Tang and Manan Roongta and Colin Cai and Jeffrey Luo and Tianjun Zhang and 
          Li Erran Li and Raluca Ada Popa and Ion Stoica et al., <code>2025.02</code></li>
          <li><i><b>Kimi k1. 5: Scaling reinforcement learning with llms</b></i>, Team et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12599" target="_blank">Link</a>]</li>
          <li><i><b>Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs</b></i>, Zhang et al., <code>2024.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/00d80722b756de0166523a87805dd00f-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <h3>External Exploration</h3>
          <li><i><b>TPO: Aligning Large Language Models with Multi-branch \& Multi-step Preference Trees</b></i>, Liao et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.12854" target="_blank">Link</a>]</li>
          <li><i><b>Thinking Preference Optimization</b></i>, Yang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13173" target="_blank">Link</a>]</li>
          <li><i><b>Kimi k1. 5: Scaling reinforcement learning with llms</b></i>, Team et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12599" target="_blank">Link</a>]</li>
          <h2>External Exploration</h2>
          <li><i><b>SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models</b></i>, Cheng et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.11605" target="_blank">Link</a>]</li>     
          <li><i><b>Forest-of-thought: Scaling test-time compute for enhancing LLM reasoning</b></i>, Bi et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.09078" target="_blank">Link</a>]</li>
          <li><i><b>Scattered Forest Search: Smarter Code Space Exploration with LLMs</b></i>, Light et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.05010" target="_blank">Link</a>]</li>
          <li><i><b>Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search</b></i>, Yao et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.18319" target="_blank">Link</a>]</li>      
          <li><i><b>Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathematical Reasoning</b></i>, Lin et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11169" target="_blank">Link</a>]</li>
          <li><i><b>Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping</b></i>, Lucas Lehnert and Sainbayar Sukhbaatar and DiJia Su and Qinqing Zheng and Paul McVay and Michael Rabbat and Yuandong Tian et al., <code>2024.07</code> [<a href="https://openreview.net/forum?id=SGoVIC0u0f" target="_blank">Link</a>]</li>
          <li><i><b>Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding</b></i>, Jiacheng Liu and Andrew Cohen and Ramakanth Pasunuru and Yejin Choi and Hannaneh Hajishirzi and Asli Celikyilmaz et al., <code>2024.07</code> [<a href="https://openreview.net/forum?id=kh9Zt2Ldmn" target="_blank">Link</a>]</li>
          <li><i><b>Making {PPO} even better: Value-Guided Monte-Carlo Tree Search decoding</b></i>, Jiacheng Liu and Andrew Cohen and Ramakanth Pasunuru and Yejin Choi and Hannaneh Hajishirzi and Asli Celikyilmaz et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=QaODpeRaOK" target="_blank">Link</a>]</li>
          <li><i><b>Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning</b></i>, Zhang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.02884" target="_blank">Link</a>]</li>
          <li><i><b>Mindstar: Enhancing math reasoning in pre-trained llms at inference time</b></i>, Kang et al., <code>2024.05</code> [<a href="https://arxiv.org/abs/2405.16265" target="_blank">Link</a>]</li>
          <li><i><b>AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning</b></i>, Xiang et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.11930" target="_blank">Link</a>]</li>
          <li><i><b>rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</b></i>, Guan et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.04519" target="_blank">Link</a>]</li>
          <li><i><b>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</b></i>, Xie et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14768" target="_blank">Link</a>]</li>
          <li><i><b>Evolving Deeper LLM Thinking</b></i>, Lee et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.09891" target="_blank">Link</a>]</li>
          <li><i><b>Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models</b></i>, Kim et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11881" target="_blank">Link</a>]</li>
          <li><i><b>On the Empirical Complexity of Reasoning and Planning in {LLM}s</b></i>, Kang et al., <code>2024.11</code> [<a href="https://aclanthology.org/2024.findings-emnlp.164/" target="_blank">Link</a>]</li>
          <li><i><b>Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing</b></i>, Tian et al., <code>2024.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/5e5853f35164e434015716a8c2a66543-Paper-Conference.pdf" target="_blank">Link</a>]</li>
          <li><i><b>{PATHFINDER}: Guided Search over Multi-Step Reasoning Paths</b></i>, Olga Golovneva and Sean O'Brien and Ramakanth Pasunuru and Tianlu Wang and Luke Zettlemoyer and Maryam Fazel-Zarandi and Asli Celikyilmaz et al., <code>2023.12</code> [<a href="https://openreview.net/forum?id=5TsfEEwRsu" target="_blank">Link</a>]</li>
          <li><i><b>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</b></i>, Denny Zhou and Nathanael Sch{\"a}rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V Le and Ed H. Chi et al., <code>2023.02</code> [<a href="https://openreview.net/forum?id=WZH7099tgfM" target="_blank">Link</a>]</li>
          <li><i><b>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</b></i>, Yao et al., <code>2023.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf" target="_blank">Link</a>]</li>
          <li><i><b>Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation, Not Discrimination</b></i>, Chen et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.17820" target="_blank">Link</a>]</li>        
          <li><i><b>Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling</b></i>, Qiu et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.16033" target="_blank">Link</a>]</li>
          <li><i><b>Tree of Uncertain Thoughts Reasoning for Large Language Models</b></i>, Mo et al., <code>2024.04</code></li>
          <li><i><b>Demystifying chains, trees, and graphs of thoughts</b></i>, Besta et al., <code>2024.01</code> [<a href="https://arxiv.org/abs/2401.14295" target="_blank">Link</a>]</li>
          <li><i><b>A Roadmap to Guide the Integration of LLMs in Hierarchical Planning</b></i>, Puerta-Merino et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.08068" target="_blank">Link</a>]</li>
          <li><i><b>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</b></i>, Besta et al., <code>2024.03</code> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/29720" target="_blank">Link</a>]</li>
          <li><i><b>Atom of Thoughts for Markov LLM Test-Time Scaling</b></i>, Teng et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12018" target="_blank">Link</a>]</li>
          <li><i><b>CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models</b></i>, Li et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.04329" target="_blank">Link</a>]</li>
          <li><i><b>Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code Generation and Execution in Complex Task Handling</b></i>, Ni et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.15305" target="_blank">Link</a>]</li>
          <li><i><b>AlphaMath Almost Zero: Process Supervision without Process</b></i>, Guoxin Chen and Minpeng Liao and Chengxi Li and Kai Fan et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=VaXnxQ3UKo" target="_blank">Link</a>]</li>
          <li><i><b>Marco-o1: Towards open reasoning models for open-ended solutions</b></i>, Zhao et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.14405" target="_blank">Link</a>]</li>
          <li><i><b>Proposing and solving olympiad geometry with guided tree search</b></i>, Zhang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.10673" target="_blank">Link</a>]</li>
          <li><i><b>Technical report: Enhancing llm reasoning with reward-guided tree search</b></i>, Jiang et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.11694" target="_blank">Link</a>]</li>
          <li><i><b>SRA-MCTS: Self-driven Reasoning Aurmentation with Monte Carlo Tree Search for Enhanced Code Generation</b></i>, Xu et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.11053" target="_blank">Link</a>]</li>       
          <li><i><b>Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models</b></i>, Puerto et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.03181" target="_blank">Link</a>]</li> 
          <li><i><b>Aflow: Automating agentic workflow generation</b></i>, Zhang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.10762" target="_blank">Link</a>]</li>
          <li><i><b>RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation</b></i>, Li et al., <code>2024.09</code> [<a href="https://arxiv.org/abs/2409.09584" target="_blank">Link</a>]</li>
          <li><i><b>Monte carlo tree search boosts reasoning via iterative preference learning</b></i>, Xie et al., <code>2024.05</code> [<a href="https://arxiv.org/abs/2405.00451" target="_blank">Link</a>]</li>
          <li><i><b>GPT-Guided Monte Carlo Tree Search for Symbolic Regression in Financial Fraud Detection</b></i>, Kadam et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.04459" target="_blank">Link</a>]</li>
          <li><i><b>Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning</b></i>, Jiang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.17397" target="_blank">Link</a>]</li>
          <li><i><b>Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search</b></i>, Li et al., <code>2025.12</code> [<a href="https://openreview.net/forum?id=OupEEi1341" target="_blank">Link</a>]</li>
          <li><i><b>Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design</b></i>, Zheng et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.08603" target="_blank">Link</a>]</li>
          <li><i><b>Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning</b></i>, Wang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.06508" target="_blank">Link</a>]</li>
          <li><i><b>MC-NEST--Enhancing Mathematical Reasoning in Large Language Models with a Monte Carlo Nash Equilibrium Self-Refine Tree</b></i>, Rabby et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.15645" target="_blank">Link</a>]</li>
          <li><i><b>CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning</b></i>, Pan et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.02390" target="_blank">Link</a>]</li>
          <li><i><b>Reasoning with language model is planning with world model</b></i>, Hao et al., <code>2023.05</code> [<a href="https://arxiv.org/abs/2305.14992" target="_blank">Link</a>]</li>
          <li><i><b>Large language models as commonsense knowledge for large-scale task planning</b></i>, Zhao et al., <code>2023.12</code></li>
          <li><i><b>Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models</b></i>, Andy Zhou and Kai Yan and Michal Shlapentokh-Rothman and Haohan Wang and Yu-Xiong Wang et al., <code>2024.05</code> [<a href="https://openreview.net/forum?id=njwv9BsGHF" target="_blank">Link</a>]</li>
          <li><i><b>Tree search for language model agents</b></i>, Koh et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.01476" target="_blank">Link</a>]</li>
          <li><i><b>Strategist: Learning Strategic Skills by {LLM}s via Bi-Level Tree Search</b></i>, Jonathan Light and Min Cai and Weiqin Chen and Guanzhi Wang and Xiusi Chen and Wei Cheng and Yisong Yue and Ziniu Hu et al., <code>2024.06</code> [<a href="https://openreview.net/forum?id=UHWBmZuJPF" target="_blank">Link</a>]</li>
          <li><i><b>Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection agents</b></i>, He et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.00430" target="_blank">Link</a>]</li>
          <li><i><b>Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models</b></i>, Wang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.20007" target="_blank">Link</a>]</li>
          <li><i><b>Deliberate reasoning for llms as structure-aware planning with accurate world model</b></i>, Xiong et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.03136" target="_blank">Link</a>]</li>
          <li><i><b>VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data</b></i>, Zeng et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06737" target="_blank">Link</a>]</li>
          </ul>
        </div>
      </ul>
    </div>
    <div class="columns is-centered">
      <div class="column is-9">
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
