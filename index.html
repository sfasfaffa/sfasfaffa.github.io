<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>



                  <section class="hero">
                    <div class="hero-body">
                      <div class="container is-max-desktop">
                        <div class="columns is-centered">
                          <div class="column has-text-centered">
                            <h1 class="title is-1 publication-title">Towards Reasoning Era: A Survey of Long Chain-of-Thought</h1>
                            <div class="is-size-5 publication-authors">
                              <!-- Paper authors -->
                              <span class="author-block">
                                <a href="https://lightchen233.github.io/" target="_blank">Qiguang Chen</a><sup>*</sup>,</span>
                                <span class="author-block">
                                  <a href="https://faculty.csu.edu.cn/qinlibo/zh_CN/index.htm" target="_blank">Libo Qin</a>,</span>
                                  <span class="author-block">
                                    <a href="https://github.com/FarzoneILIN" target="_blank">Jinhao Liu</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://github.com/sfasfaffa" target="_blank">Dengyun Peng</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://github.com/GoatCsu" target="_blank">Te Gao</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://aaron617.github.io/" target="_blank">MengKang Hu</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://chewanxiang.com/" target="_blank">Wanxiang Che</a>
                                  </span>
                                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Harbin Institude of Technology</span>
                    <!-- <span class="author-block">Harbin Institude of Technology<br>Conferance name and year</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in logical reasoning tasks are often attributed to test-time scaling, with many researchers suggesting that increasing model capacity to process longer reasoning sequences improves performance. However, this idea is challenged in simpler tasks, such as commonsense reasoning and basic mathematics, where test-time scaling may lead to “overthinking” that hampers model performance. This paradox remains underexplored, with existing research limited by two main shortcomings: a failure to distinguish between Long Chains of Thought (Long CoT) and Short Chains of Thought (Short CoT), and the absence of a comprehensive review on the topic. To address these issues, this survey first distinguishes between Long CoT and Short CoT, introducing a new taxonomy to categorize these reasoning paradigms. We examine the key characteristics of Long CoT—Deep Reasoning, Extensive Exploration, and Feasible Reflection—and highlight how these features enable deeper and more efficient reasoning compared to the shallow, redundancy-prone Short CoT. Our review synthesizes the current state of Long CoT research, identifies critical gaps, and suggests future research directions. We also address challenges in Long CoT, such as multi-modal reasoning, efficiency, and knowledge integration, and recommend resources, including open-source software, corpora, and key publications, to support further studies. Through this survey, we aim to offer a unified perspective on Long CoT, propose strategies to overcome existing limitations, and inspire future research to push the boundaries of logical reasoning in artificial intelligence.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="section">
  <div class="container">
    <div class="has-text-centered">
      <h1 class="title is-2">Paper List</h1>
      <p class="subtitle is-4"></p>
    </div>
    <div class="content">
      <div id="type" style="display:none;">Paper List</div>
      <ul>
        <div id="mllm-paperlist">
          <!-- Deep Reasoning -->
          <h2>Deep Reasoning</h2>

          <!-- Deep Reasoning Execution -->
          <h3>Deep Reasoning Execution</h3>
          <ul>
            <li><i><b>Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models</b></i>, Unknown Authors <code>2024.06</code></li>
            <li><i><b>SKIntern: Internalizing Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models</b></i>, Unknown Authors <code>2025.01</code></li>
            <li><i><b>Guiding language model reasoning with planning tokens</b></i>, Unknown Authors <code>2023.10</code></li>
            <li><i><b>Efficient Reasoning with Hidden Thinking</b></i>, Unknown Authors <code>2025.01</code></li>
            <li><i><b>Scalable Language Models with Posterior Inference of Latent Thought Vectors</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</b></i>, Unknown Authors <code>2022.11</code></li>
            <li><i><b>Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages</b></i>, Unknown Authors <code>2023.07</code></li>
            <li><i><b>{A}uto{CAP}: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought</b></i>, Unknown Authors <code>2024.08</code></li>
            <li><i><b>CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</b></i>, Unknown Authors <code>2023.11</code></li>
            <li><i><b>Chain of Code: Reasoning with a Language Model-Augmented Code Emulator</b></i>, Unknown Authors <code>2024.07</code></li>
            <li><i><b>Lean-star: Learning to interleave thinking and proving</b></i>, Unknown Authors <code>2024.07</code></li>
            <li><i><b>Quiet-star: Language models can teach themselves to think before speaking</b></i>, Unknown Authors <code>2024.03</code></li>
            <li><i><b>Training large language models to reason in a continuous latent space</b></i>, Unknown Authors <code>2024.12</code></li>
          </ul>

          <!-- Deep Reasoning Learning -->
          <h3>Deep Reasoning Learning</h3>
          <ul>
            <li><i><b>Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>STeCa: Step-level Trajectory Calibration for LLM Agent Learning</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>V-star: Training verifiers for self-taught reasoners</b></i>, Unknown Authors <code>2024.02</code></li>
            <li><i><b>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</b></i>, Unknown Authors <code>2024.04</code></li>
            <li><i><b>FastMCTS: A Simple Sampling Strategy for Data Synthesis</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models</b></i>, Unknown Authors <code>2024.06</code></li>
            <li><i><b>LLMs Can Teach Themselves to Better Predict the Future</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Unknown Authors <code>2025.01</code></li>
            <li><i><b>Sft memorizes, rl generalizes: A comparative study of foundation model post-training</b></i>, Unknown Authors <code>2025.01</code></li>
            <li><i><b>Demystifying Long Chain-of-Thought Reasoning in LLMs</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Advancing Math Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages</b></i>, Unknown Authors <code>2025.01</code></li>
            <li><i><b>Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Large Language Models Are Reasoning Teachers</b></i>, Unknown Authors <code>2023.07</code></li>
            <li><i><b>Open R1</b></i>, Unknown Authors <code>2025.01</code></li>
            <li><i><b>Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus</b></i>, Unknown Authors <code>2024.09</code></li>
            <li><i><b>System-2 Mathematical Reasoning via Enriched Instruction Tuning</b></i>, Unknown Authors <code>2024.12</code></li>
            <li><i><b>Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes</b></i>, Unknown Authors <code>2024.03</code></li>
            <li><i><b>Distillation Scaling Laws</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>O1 Replication Journey--Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?</b></i>, Unknown Authors <code>2024.11</code></li>
            <li><i><b>Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems</b></i>, Unknown Authors <code>2024.12</code></li>
            <li><i><b>Openai o1 system card</b></i>, Unknown Authors <code>2024.12</code></li>
            <li><i><b>LIMO: Less is More for Reasoning</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?</b></i>, Unknown Authors <code>2025.01</code></li>
            <li><i><b>Star: Bootstrapping reasoning with reasoning</b></i>, Unknown Authors <code>2022.11</code></li>
            <li><i><b>BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation</b></i>, Unknown Authors <code>2025.02</code></li>
            <li><i><b>Reinforced self-training (rest) for language modeling</b></i>, Unknown Authors <code>2023.08</code></li>
          </ul>
        </div>
      </ul>
    </div>
    <div class="columns is-centered">
      <div class="column is-9">
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
