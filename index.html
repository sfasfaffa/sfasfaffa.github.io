<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>



                  <section class="hero">
                    <div class="hero-body">
                      <div class="container is-max-desktop">
                        <div class="columns is-centered">
                          <div class="column has-text-centered">
                            <h1 class="title is-1 publication-title">Towards Reasoning Era: A Survey of Long Chain-of-Thought</h1>
                            <div class="is-size-5 publication-authors">
                              <!-- Paper authors -->
                              <span class="author-block">
                                <a href="https://lightchen233.github.io/" target="_blank">Qiguang Chen</a><sup>*</sup>,</span>
                                <span class="author-block">
                                  <a href="https://faculty.csu.edu.cn/qinlibo/zh_CN/index.htm" target="_blank">Libo Qin</a>,</span>
                                  <span class="author-block">
                                    <a href="https://github.com/FarzoneILIN" target="_blank">Jinhao Liu</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://github.com/sfasfaffa" target="_blank">Dengyun Peng</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://github.com/GoatCsu" target="_blank">Te Gao</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://aaron617.github.io/" target="_blank">MengKang Hu</a>,
                                  </span>
                                  <span class="author-block">
                                    <a href="https://chewanxiang.com/" target="_blank">Wanxiang Che</a>
                                  </span>
                                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Harbin Institude of Technology</span>
                    <!-- <span class="author-block">Harbin Institude of Technology<br>Conferance name and year</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in logical reasoning tasks are often attributed to test-time scaling, with many researchers suggesting that increasing model capacity to process longer reasoning sequences improves performance. However, this idea is challenged in simpler tasks, such as commonsense reasoning and basic mathematics, where test-time scaling may lead to “overthinking” that hampers model performance. This paradox remains underexplored, with existing research limited by two main shortcomings: a failure to distinguish between Long Chains of Thought (Long CoT) and Short Chains of Thought (Short CoT), and the absence of a comprehensive review on the topic. To address these issues, this survey first distinguishes between Long CoT and Short CoT, introducing a new taxonomy to categorize these reasoning paradigms. We examine the key characteristics of Long CoT—Deep Reasoning, Extensive Exploration, and Feasible Reflection—and highlight how these features enable deeper and more efficient reasoning compared to the shallow, redundancy-prone Short CoT. Our review synthesizes the current state of Long CoT research, identifies critical gaps, and suggests future research directions. We also address challenges in Long CoT, such as multi-modal reasoning, efficiency, and knowledge integration, and recommend resources, including open-source software, corpora, and key publications, to support further studies. Through this survey, we aim to offer a unified perspective on Long CoT, propose strategies to overcome existing limitations, and inspire future research to push the boundaries of logical reasoning in artificial intelligence.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="section">
  <div class="container">
    <div class="has-text-centered">
      <h1 class="title is-2">Paper List</h1>
      <p class="subtitle is-4"></p>
    </div>
    <div class="content">
      <div id="type" style="display:none;">Paper List</div>
      <ul>
        <div id="mllm-paperlist">

        <h2>Deep Reasoning</h2>
        <h3>Deep Reasoning Execution</h3>
        <ul>
        <li><i><b>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</b></i>, Wei et al., <code>2022.11</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <li><i><b>Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages</b></i>, Qin et al., <code>2023.07</code></li>
        <li><i><b>Guiding language model reasoning with planning tokens</b></i>, Wang et al., <code>2023.10</code> [<a href="https://arxiv.org/abs/2310.05707" target="_blank">Link</a>]</li>
        <li><i><b>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</b></i>, Wenhu Chen and Xueguang Ma and Xinyi Wang and William 
        W. Cohen et al., <code>2023.11</code> [<a href="https://openreview.net/forum?id=YfZ4ZPt8zd" target="_blank">Link</a>]</li>
        <li><i><b>Quiet-star: Language models can teach themselves to think before speaking</b></i>, Zelikman et al., <code>2024.03</code> [<a href="https://arxiv.org/abs/2403.09629" 
        target="_blank">Link</a>]</li>
        <li><i><b>Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models</b></i>, Xu et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.11736" target="_blank">Link</a>]</li>
        <li><i><b>Chain of Code: Reasoning with a Language Model-Augmented Code Emulator</b></i>, Li et al., <code>2024.07</code> [<a href="https://proceedings.mlr.press/v235/li24ar.html" target="_blank">Link</a>]</li>
        <li><i><b>Lean-star: Learning to interleave thinking and proving</b></i>, Lin et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.10040" target="_blank">Link</a>]</li>
        <li><i><b>{A}uto{CAP}: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought</b></i>, Zhang et al., <code>2024.08</code> [<a href="https://aclanthology.org/2024.findings-acl.546/" target="_blank">Link</a>]</li>
        <li><i><b>Training large language models to reason in a continuous latent space</b></i>, Hao et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.06769" target="_blank">Link</a>]</li>
        <li><i><b>SKIntern: Internalizing Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models</b></i>, Liao et al., <code>2025.01</code></li>
        <li><i><b>Efficient Reasoning with Hidden Thinking</b></i>, Shen et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.19201" target="_blank">Link</a>]</li>       
        <li><i><b>Scalable Language Models with Posterior Inference of Latent Thought Vectors</b></i>, Kong et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01567" target="_blank">Link</a>]</li>
        <li><i><b>Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13842" target="_blank">Link</a>]</li>
        <li><i><b>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</b></i>, Geiping et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05171" target="_blank">Link</a>]</li>
        <li><i><b>CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07316" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>Deep Reasoning Learning</h3>
        <ul>
        <li><i><b>Star: Bootstrapping reasoning with reasoning</b></i>, Zelikman et al., <code>2022.11</code></li>
        <li><i><b>Large Language Models Are Reasoning Teachers</b></i>, Ho et al., <code>2023.07</code> [<a href="https://aclanthology.org/2023.acl-long.830/" target="_blank">Link</a>]</li>
        <li><i><b>Reinforced self-training (rest) for language modeling</b></i>, Gulcehre et al., <code>2023.08</code> [<a href="https://arxiv.org/abs/2308.08998" target="_blank">Link</a>]</li>
        <li><i><b>V-star: Training verifiers for self-taught reasoners</b></i>, Hosseini et al., <code>2024.02</code> [<a href="https://arxiv.org/abs/2402.06457" target="_blank">Link</a>]</li>
        <li><i><b>Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes</b></i>, Chen et al., <code>2024.03</code> [<a href="https://arxiv.org/abs/2403.00800" target="_blank">Link</a>]</li>
        <li><i><b>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</b></i>, Singh et al., <code>2024.04</code></li>
        <li><i><b>Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models</b></i>, Xu et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.11736" target="_blank">Link</a>]</li>
        <li><i><b>Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus</b></i>, Morishita et al., <code>2024.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/8678da90126aa58326b2fc0254b33a8c-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <li><i><b>O1 Replication Journey--Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?</b></i>, Huang et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.16489" target="_blank">Link</a>]</li>
        <li><i><b>System-2 Mathematical Reasoning via Enriched Instruction Tuning</b></i>, Cai et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16964" target="_blank">Link</a>]</li>
        <li><i><b>Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems</b></i>, Min et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.09413" target="_blank">Link</a>]</li>
        <li><i><b>Openai o1 system card</b></i>, Jaech et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16720" target="_blank">Link</a>]</li>
        <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
        <li><i><b>Sft memorizes, rl generalizes: A comparative study of foundation model post-training</b></i>, Chu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17161" target="_blank">Link</a>]</li>
        <li><i><b>Advancing Math Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages</b></i>, Chen et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.14002" target="_blank">Link</a>]</li>
        <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
        <li><i><b>Sft memorizes, rl generalizes: A comparative study of foundation model post-training</b></i>, Chu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17161" target="_blank">Link</a>]</li>
        <li><i><b>Open R1</b></i>, Huggingface Team et al., <code>2025.01</code></li>
        <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
        <li><i><b>RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?</b></i>, Xu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.11284" target="_blank">Link</a>]</li>
        <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
        <li><i><b>Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models</b></i>, Chijiwa et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12776" target="_blank">Link</a>]</li>
        <li><i><b>STeCa: Step-level Trajectory Calibration for LLM Agent Learning</b></i>, Wang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14276" target="_blank">Link</a>]</li>
        <li><i><b>FastMCTS: A Simple Sampling Strategy for Data Synthesis</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11476" target="_blank">Link</a>]</li>
        <li><i><b>LLMs Can Teach Themselves to Better Predict the Future</b></i>, Turtel et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05253" target="_blank">Link</a>]</li>
        <li><i><b>Demystifying Long Chain-of-Thought Reasoning in LLMs</b></i>, Yeo et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03373" target="_blank">Link</a>]</li>
        <li><i><b>Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls</b></i>, Wang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11183" target="_blank">Link</a>]</li>
        <li><i><b>Distillation Scaling Laws</b></i>, Busbridge et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.08606" target="_blank">Link</a>]</li>
        <li><i><b>Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization</b></i>, Yao et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.04667" target="_blank">Link</a>]</li>
        <li><i><b>LIMO: Less is More for Reasoning</b></i>, Ye et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03387" target="_blank">Link</a>]</li>
        <li><i><b>BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation</b></i>, Pang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03860" 
        target="_blank">Link</a>]</li>
        </ul>
        <h2>Extensive Exploration</h2>
        <h3>Exploration Scaling</h3>
        <ul>
        <li><i><b>Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation</b></i>, Lyzhov et al., <code>2020.08</code> [<a href="https://proceedings.mlr.press/v124/lyzhov20a.html" target="_blank">Link</a>]</li>
        <li><i><b>Making large language models better reasoners with step-aware verifier</b></i>, Li et al., <code>2022.06</code> [<a href="https://arxiv.org/abs/2206.02336" target="_blank">Link</a>]</li>
        <li><i><b>Complexity-Based Prompting for Multi-step Reasoning</b></i>, Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot et al., <code>2023.02</code> [<a href="https://openreview.net/forum?id=yf1icZHC-l9" target="_blank">Link</a>]</li>
        <li><i><b>Self-Consistency Improves Chain of Thought Reasoning in Language Models</b></i>, Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou et al., <code>2023.02</code> [<a href="https://openreview.net/forum?id=1PL1NIMMrw" target="_blank">Link</a>]</li>
        <li><i><b>Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages</b></i>, Qin et al., <code>2023.07</code></li>
        <li><i><b>Don't Trust: Verify -- Grounding {LLM} Quantitative Reasoning with Autoformalization</b></i>, Jin Peng Zhou and Charles E Staats and Wenda Li and Christian Szegedy and Kilian Q Weinberger and Yuhuai Wu et al., <code>2024.01</code> [<a href="https://openreview.net/forum?id=V5tdi14ple" target="_blank">Link</a>]</li>
        <li><i><b>Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision</b></i>, Wang et al., <code>2024.02</code> [<a href="https://arxiv.org/abs/2402.02658" target="_blank">Link</a>]</li>
        <li><i><b>Improve Mathematical Reasoning in Language Models by Automated Process Supervision</b></i>, Luo et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.06592" target="_blank">Link</a>]</li>
        <li><i><b>The Impact of Reasoning Step Length on Large Language Models</b></i>, Jin et al., <code>2024.08</code> [<a href="https://aclanthology.org/2024.findings-acl.108/" target="_blank">Link</a>]</li>
        <li><i><b>Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models</b></i>, Wu et al., <code>2024.08</code> [<a href="https://arxiv.org/abs/2408.00724" target="_blank">Link</a>]</li>
        <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
        <li><i><b>MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning</b></i>, Chen et al., <code>2024.09</code> [<a href="https://arxiv.org/abs/2409.12147" target="_blank">Link</a>]</li>
        <li><i><b>Scaling llm inference with optimized sample compute allocation</b></i>, Zhang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.22480" target="_blank">Link</a>]</li>
        <li><i><b>Scaling Inference Computation: Compute-Optimal Inference for Problem-Solving with Language Models</b></i>, Yangzhen Wu and Zhiqing Sun and Shanda Li and Sean Welleck and Yiming Yang et al., <code>2024.10</code> [<a href="https://openreview.net/forum?id=j7DZWSc8qu" target="_blank">Link</a>]</li>
        <li><i><b>Beyond examples: High-level automated reasoning paradigm in in-context learning via mcts</b></i>, Wu et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.18478" target="_blank">Link</a>]</li>
        <li><i><b>From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models</b></i>, Sean Welleck and Amanda Bertsch and Matthew Finlayson and Hailey Schoelkopf and Alex Xie and Graham Neubig and Ilia Kulikov and Zaid Harchaoui et al., <code>2024.11</code> [<a href="https://openreview.net/forum?id=eskQMcIbMS" target="_blank">Link</a>]</li>
        <li><i><b>Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information</b></i>, Zhang et al., <code>2024.11</code> [<a href="https://aclanthology.org/2024.findings-emnlp.388/" target="_blank">Link</a>]</li>
        <li><i><b>A simple and provable scaling law for the test-time compute of large language models</b></i>, Chen et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.19477" target="_blank">Link</a>]</li>
        <li><i><b>Openai o1 system card</b></i>, Jaech et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16720" target="_blank">Link</a>]</li>
        <li><i><b>Lachesis: Predicting LLM Inference Accuracy using Structural Properties of Reasoning Paths</b></i>, Kim et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.08281" target="_blank">Link</a>]</li>
        <li><i><b>Seed-cts: Unleashing the power of tree search for superior performance in competitive coding tasks</b></i>, Wang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.12544" target="_blank">Link</a>]</li>
        <li><i><b>Inference Scaling vs Reasoning: An Empirical Analysis of Compute-Optimal LLM Problem-Solving</b></i>, AbdElhameed et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16260" target="_blank">Link</a>]</li>
        <li><i><b>s1: Simple test-time scaling</b></i>, Muennighoff et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.19393" target="_blank">Link</a>]</li>
        <li><i><b>From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning</b></i>, Li et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.11877" target="_blank">Link</a>]</li>
        <li><i><b>Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective</b></i>, Yu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.11110" target="_blank">Link</a>]</li>
        <li><i><b>Test-time Computing: from System-1 Thinking to System-2 Thinking</b></i>, Ji et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.02497" target="_blank">Link</a>]</li>
        <li><i><b>Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers</b></i>, Raza et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.16961" target="_blank">Link</a>]</li>
        <li><i><b>SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling</b></i>, Chen et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.19306" target="_blank">Link</a>]</li>
        <li><i><b>DnA-Eval: Enhancing Large Language Model Evaluation through Decomposition and Aggregation</b></i>, Li et al., <code>2025.01</code></li>
        <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
        <li><i><b>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</b></i>, Geiping et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05171" target="_blank">Link</a>]</li>
        <li><i><b>When More is Less: Understanding Chain-of-Thought Length in LLMs</b></i>, Wu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07266" target="_blank">Link</a>]</li>
        <li><i><b>Confidence Improves Self-Consistency in LLMs</b></i>, Taubenfeld et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06233" target="_blank">Link</a>]</li>
        <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
        <li><i><b>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</b></i>, Liu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06703" target="_blank">Link</a>]</li>
        <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
        <li><i><b>Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?</b></i>, Zeng et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12215" target="_blank">Link</a>]</li>
        <li><i><b>Optimizing Temperature for Language Models with Multi-Sample Inference</b></i>, Du et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05234" target="_blank">Link</a>]</li>
        <li><i><b>Bag of Tricks for Inference-time Computation of LLM Reasoning</b></i>, Liu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07191" target="_blank">Link</a>]</li>
        <li><i><b>Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights</b></i>, Parashar et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12521" target="_blank">Link</a>]</li>
        <li><i><b>Examining False Positives under Inference Scaling for Mathematical Reasoning</b></i>, Wang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06217" target="_blank">Link</a>]</li>
        <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
        <li><i><b>Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07803" target="_blank">Link</a>]</li>
        <li><i><b>PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models</b></i>, Anderson et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01584" target="_blank">Link</a>]</li>
        <li><i><b>Scaling Test-Time Compute Without Verification or RL is Suboptimal</b></i>, Setlur et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12118" target="_blank">Link</a>]</li>
        <li><i><b>Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification</b></i>, Zhao et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01839" target="_blank">Link</a>]</li>
        <li><i><b>ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification</b></i>, Lee et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14565" target="_blank">Link</a>]</li>
        <li><i><b>S*: Test Time Scaling for Code Generation</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14382" target="_blank">Link</a>]</li>        
        <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
        <li><i><b>Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs</b></i>, Wu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.10858" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>External Exploration</h3>
        <ul>
        <li><i><b>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</b></i>, Denny Zhou and Nathanael Sch{\"a}rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V Le and Ed H. Chi et al., <code>2023.02</code> [<a href="https://openreview.net/forum?id=WZH7099tgfM" target="_blank">Link</a>]</li>
        <li><i><b>Reasoning with language model is planning with world model</b></i>, Hao et al., <code>2023.05</code> [<a href="https://arxiv.org/abs/2305.14992" target="_blank">Link</a>]</li>
        <li><i><b>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</b></i>, Yao et al., <code>2023.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <li><i><b>{PATHFINDER}: Guided Search over Multi-Step Reasoning Paths</b></i>, Olga Golovneva and Sean O'Brien and Ramakanth Pasunuru and Tianlu Wang and Luke Zettlemoyer and 
        Maryam Fazel-Zarandi and Asli Celikyilmaz et al., <code>2023.12</code> [<a href="https://openreview.net/forum?id=5TsfEEwRsu" target="_blank">Link</a>]</li>
        <li><i><b>Large language models as commonsense knowledge for large-scale task planning</b></i>, Zhao et al., <code>2023.12</code></li>
        <li><i><b>Demystifying chains, trees, and graphs of thoughts</b></i>, Besta et al., <code>2024.01</code> [<a href="https://arxiv.org/abs/2401.14295" target="_blank">Link</a>]</li>
        <li><i><b>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</b></i>, Besta et al., <code>2024.03</code> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/29720" target="_blank">Link</a>]</li>
        <li><i><b>Tree of Uncertain Thoughts Reasoning for Large Language Models</b></i>, Mo et al., <code>2024.04</code></li>
        <li><i><b>Mindstar: Enhancing math reasoning in pre-trained llms at inference time</b></i>, Kang et al., <code>2024.05</code> [<a href="https://arxiv.org/abs/2405.16265" target="_blank">Link</a>]</li>
        <li><i><b>Monte carlo tree search boosts reasoning via iterative preference learning</b></i>, Xie et al., <code>2024.05</code> [<a href="https://arxiv.org/abs/2405.00451" target="_blank">Link</a>]</li>
        <li><i><b>Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models</b></i>, Andy Zhou and Kai Yan and Michal Shlapentokh-Rothman and Haohan Wang and Yu-Xiong Wang et al., <code>2024.05</code> [<a href="https://openreview.net/forum?id=njwv9BsGHF" target="_blank">Link</a>]</li>
        <li><i><b>Strategist: Learning Strategic Skills by {LLM}s via Bi-Level Tree Search</b></i>, Jonathan Light and Min Cai and Weiqin Chen and Guanzhi Wang and Xiusi Chen and Wei 
        Cheng and Yisong Yue and Ziniu Hu et al., <code>2024.06</code> [<a href="https://openreview.net/forum?id=UHWBmZuJPF" target="_blank">Link</a>]</li>
        <li><i><b>Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping</b></i>, Lucas Lehnert and Sainbayar Sukhbaatar and DiJia Su and Qinqing Zheng and Paul McVay and Michael Rabbat and Yuandong Tian et al., <code>2024.07</code> [<a href="https://openreview.net/forum?id=SGoVIC0u0f" target="_blank">Link</a>]</li>
        <li><i><b>Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding</b></i>, Jiacheng Liu and Andrew Cohen and Ramakanth Pasunuru and Yejin Choi and Hannaneh Hajishirzi and Asli Celikyilmaz et al., <code>2024.07</code> [<a href="https://openreview.net/forum?id=kh9Zt2Ldmn" target="_blank">Link</a>]</li>
        <li><i><b>Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models</b></i>, Puerto et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.03181" target="_blank">Link</a>]</li>
        <li><i><b>Tree search for language model agents</b></i>, Koh et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.01476" target="_blank">Link</a>]</li>
        <li><i><b>Making {PPO} even better: Value-Guided Monte-Carlo Tree Search decoding</b></i>, Jiacheng Liu and Andrew Cohen and Ramakanth Pasunuru and Yejin Choi and Hannaneh Hajishirzi and Asli Celikyilmaz et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=QaODpeRaOK" target="_blank">Link</a>]</li>
        <li><i><b>Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing</b></i>, Tian et al., <code>2024.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/5e5853f35164e434015716a8c2a66543-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <li><i><b>AlphaMath Almost Zero: Process Supervision without Process</b></i>, Guoxin Chen and Minpeng Liao and Chengxi Li and Kai Fan et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=VaXnxQ3UKo" target="_blank">Link</a>]</li>
        <li><i><b>RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation</b></i>, Li et al., <code>2024.09</code> [<a href="https://arxiv.org/abs/2409.09584" target="_blank">Link</a>]</li>
        <li><i><b>Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning</b></i>, Zhang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.02884" target="_blank">Link</a>]</li>
        <li><i><b>Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation, Not Discrimination</b></i>, Chen et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.17820" target="_blank">Link</a>]</li>
        <li><i><b>Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling</b></i>, Qiu et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.16033" target="_blank">Link</a>]</li>
        <li><i><b>Aflow: Automating agentic workflow generation</b></i>, Zhang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.10762" target="_blank">Link</a>]</li> 
        <li><i><b>Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning</b></i>, Wang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.06508" target="_blank">Link</a>]</li>
        <li><i><b>Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models</b></i>, Wang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.20007" target="_blank">Link</a>]</li>
        <li><i><b>Deliberate reasoning for llms as structure-aware planning with accurate world model</b></i>, Xiong et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.03136" target="_blank">Link</a>]</li>
        <li><i><b>Scattered Forest Search: Smarter Code Space Exploration with LLMs</b></i>, Light et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.05010" target="_blank">Link</a>]</li>
        <li><i><b>AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning</b></i>, Xiang et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.11930" target="_blank">Link</a>]</li>
        <li><i><b>On the Empirical Complexity of Reasoning and Planning in {LLM}s</b></i>, Kang et al., <code>2024.11</code> [<a href="https://aclanthology.org/2024.findings-emnlp.164/" target="_blank">Link</a>]</li>
        <li><i><b>CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models</b></i>, Li et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.04329" target="_blank">Link</a>]</li>
        <li><i><b>Marco-o1: Towards open reasoning models for open-ended solutions</b></i>, Zhao et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.14405" target="_blank">Link</a>]</li>
        <li><i><b>Technical report: Enhancing llm reasoning with reward-guided tree search</b></i>, Jiang et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.11694" target="_blank">Link</a>]</li>
        <li><i><b>SRA-MCTS: Self-driven Reasoning Aurmentation with Monte Carlo Tree Search for Enhanced Code Generation</b></i>, Xu et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.11053" target="_blank">Link</a>]</li>
        <li><i><b>GPT-Guided Monte Carlo Tree Search for Symbolic Regression in Financial Fraud Detection</b></i>, Kadam et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.04459" target="_blank">Link</a>]</li>
        <li><i><b>MC-NEST--Enhancing Mathematical Reasoning in Large Language Models with a Monte Carlo Nash Equilibrium Self-Refine Tree</b></i>, Rabby et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.15645" target="_blank">Link</a>]</li>
        <li><i><b>SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models</b></i>, Cheng et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.11605" target="_blank">Link</a>]</li>
        <li><i><b>Forest-of-thought: Scaling test-time compute for enhancing LLM reasoning</b></i>, Bi et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.09078" target="_blank">Link</a>]</li>
        <li><i><b>Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search</b></i>, Yao et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.18319" target="_blank">Link</a>]</li>
        <li><i><b>Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code Generation and Execution in Complex Task Handling</b></i>, Ni et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.15305" target="_blank">Link</a>]</li>
        <li><i><b>Proposing and solving olympiad geometry with guided tree search</b></i>, Zhang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.10673" target="_blank">Link</a>]</li>
        <li><i><b>Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning</b></i>, Jiang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.17397" target="_blank">Link</a>]</li>
        <li><i><b>rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</b></i>, Guan et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.04519" target="_blank">Link</a>]</li>
        <li><i><b>Evolving Deeper LLM Thinking</b></i>, Lee et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.09891" target="_blank">Link</a>]</li>
        <li><i><b>A Roadmap to Guide the Integration of LLMs in Hierarchical Planning</b></i>, Puerta-Merino et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.08068" target="_blank">Link</a>]</li>
        <li><i><b>Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search</b></i>, Li et al., <code>2025.01</code> [<a href="https://openreview.net/forum?id=OupEEi1341" target="_blank">Link</a>]</li>
        <li><i><b>Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design</b></i>, Zheng et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.08603" target="_blank">Link</a>]</li>
        <li><i><b>Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection agents</b></i>, He et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.00430" target="_blank">Link</a>]</li>
        <li><i><b>Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathematical Reasoning</b></i>, Lin et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11169" target="_blank">Link</a>]</li>
        <li><i><b>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</b></i>, Xie et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14768" target="_blank">Link</a>]</li>
        <li><i><b>Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models</b></i>, Kim et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11881" target="_blank">Link</a>]</li>
        <li><i><b>Atom of Thoughts for Markov LLM Test-Time Scaling</b></i>, Teng et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12018" target="_blank">Link</a>]</li>
        <li><i><b>CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning</b></i>, Pan et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.02390" target="_blank">Link</a>]</li>
        <li><i><b>VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data</b></i>, Zeng et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06737" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>Internal Exploration</h3>
        <ul>
        <li><i><b>Proximal policy optimization algorithms</b></i>, Schulman et al., <code>2017.07</code> [<a href="https://arxiv.org/abs/1707.06347" target="_blank">Link</a>]</li>    
        <li><i><b>Thinking fast and slow with deep learning and tree search</b></i>, Anthony et al., <code>2017.12</code></li>
        <li><i><b>Deepseekmath: Pushing the limits of mathematical reasoning in open language models</b></i>, Shao et al., <code>2024.02</code> [<a href="https://arxiv.org/abs/2402.03300" target="_blank">Link</a>]</li>
        <li><i><b>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</b></i>, Singh et al., <code>2024.04</code></li>
        <li><i><b>AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training</b></i>, Ziyu Wan and Xidong Feng and Muning Wen and Stephen Marcus McAleer and Ying 
        Wen and Weinan Zhang and Jun Wang et al., <code>2024.05</code> [<a href="https://openreview.net/forum?id=C4OpREezgj" target="_blank">Link</a>]</li>
        <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
        <li><i><b>RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold</b></i>, Setlur et al., <code>2024.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/4b77d5b896c321a29277524a98a50215-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <li><i><b>CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning Tasks</b></i>, Wang et al., <code>2024.09</code> [<a href="https://arxiv.org/abs/2409.08642" 
        target="_blank">Link</a>]</li>
        <li><i><b>Re{ST}-{MCTS}*: {LLM} Self-Training via Process Reward Guided Tree Search</b></i>, Dan Zhang and Sining Zhoubian and Ziniu Hu and Yisong Yue and Yuxiao Dong and Jie 
        Tang et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=8rcFOqEud5" target="_blank">Link</a>]</li>
        <li><i><b>Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs</b></i>, Zhang et al., <code>2024.09</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/00d80722b756de0166523a87805dd00f-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <li><i><b>A Small Step Towards Reproducing OpenAI o1: Progress Report on the Steiner Open Source Models</b></i>, Yichao Ji et al., <code>2024.10</code> [<a href="https://medium.com/@peakji/b9a756a00855" target="_blank">Link</a>]</li>
        <li><i><b>A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications</b></i>, Xiao et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.15595" target="_blank">Link</a>]</li>
        <li><i><b>TPO: Aligning Large Language Models with Multi-branch \& Multi-step Preference Trees</b></i>, Liao et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.12854" target="_blank">Link</a>]</li>
        <li><i><b>Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability</b></i>, Lin et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.19943" target="_blank">Link</a>]</li>
        <li><i><b>Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization</b></i>, Liu et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.18279" target="_blank">Link</a>]</li>
        <li><i><b>o1-coder: an o1 replication for coding</b></i>, Zhang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.00154" target="_blank">Link</a>]</li>        
        <li><i><b>Offline Reinforcement Learning for LLM Multi-Step Reasoning</b></i>, Wang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.16145" target="_blank">Link</a>]</li>
        <li><i><b>Sft memorizes, rl generalizes: A comparative study of foundation model post-training</b></i>, Chu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17161" target="_blank">Link</a>]</li>
        <li><i><b>REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models</b></i>, Hu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.03262" 
        target="_blank">Link</a>]</li>
        <li><i><b>Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling</b></i>, Hou et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.11651" target="_blank">Link</a>]</li>
        <li><i><b>Diverse Preference Optimization</b></i>, Lanchantin et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.18101" target="_blank">Link</a>]</li>
        <li><i><b>COS (M+ O) S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via Language Models</b></i>, Materzok et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17104" target="_blank">Link</a>]</li>
        <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
        <li><i><b>Kimi k1. 5: Scaling reinforcement learning with llms</b></i>, Team et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12599" target="_blank">Link</a>]</li>
        <li><i><b>Kimi k1. 5: Scaling reinforcement learning with llms</b></i>, Team et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12599" target="_blank">Link</a>]</li>
        <li><i><b>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</b></i>, Shen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.02508" target="_blank">Link</a>]</li>
        <li><i><b>Demystifying Long Chain-of-Thought Reasoning in LLMs</b></i>, Yeo et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03373" target="_blank">Link</a>]</li>
        <li><i><b>Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning</b></i>, Vassoyan et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06533" target="_blank">Link</a>]</li>
        <li><i><b>LIMR: Less is More for RL Scaling</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11886" target="_blank">Link</a>]</li>
        <li><i><b>Training Language Models to Reason Efficiently</b></i>, Arora et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.04463" target="_blank">Link</a>]</li><li><i><b>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</b></i>, Xie et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14768" target="_blank">Link</a>]</li>
        <li><i><b>On the Emergence of Thinking in LLMs I: Searching for the Right Intuition</b></i>, Ye et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06773" target="_blank">Link</a>]</li>
        <li><i><b>Process reinforcement through implicit rewards</b></i>, Cui et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01456" target="_blank">Link</a>]</li>  
        <li><i><b>Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights</b></i>, Parashar et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12521" target="_blank">Link</a>]</li>
        <li><i><b>Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation</b></i>, Kim et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01694" target="_blank">Link</a>]</li>
        <li><i><b>Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges</b></i>, Shrestha et al., <code>2025.02</code> 
        [<a href="https://arxiv.org/abs/2502.08680" target="_blank">Link</a>]</li>
        <li><i><b>Policy Guided Tree Search for Enhanced LLM Reasoning</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06813" target="_blank">Link</a>]</li>
        <li><i><b>Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment</b></i>, Sun et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.00203" target="_blank">Link</a>]</li>
        <li><i><b>Focused-DPO: Enhancing Code Generation Through Focused Preference Optimization on Error-Prone Points</b></i>, Zhang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11475" target="_blank">Link</a>]</li>
        <li><i><b>Reasoning with Reinforced Functional Token Tuning</b></i>, Zhang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13389" target="_blank">Link</a>]</li>
        <li><i><b>Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning</b></i>, Lyu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06781" target="_blank">Link</a>]</li>
        <li><i><b>Competitive Programming with Large Reasoning Models</b></i>, El-Kishky et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06807" target="_blank">Link</a>]</li>
        <li><i><b>DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL</b></i>, Michael Luo and Sijun Tan and Justin Wong and Xiaoxiang Shi and William Y. Tang and Manan 
        Roongta and Colin Cai and Jeffrey Luo and Tianjun Zhang and Li Erran Li and Raluca Ada Popa and Ion Stoica et al., <code>2025.02</code></li>
        <li><i><b>Thinking Preference Optimization</b></i>, Yang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13173" target="_blank">Link</a>]</li>

        </ul>
        <h2>Feasible Reflection</h2>
        <h3>Feedback</h3>
        <ul>
        <li><i><b>Concrete problems in AI safety</b></i>, Amodei et al., <code>2016.06</code> [<a href="https://arxiv.org/abs/1606.06565" target="_blank">Link</a>]</li>
        <li><i><b>The effects of reward misspecification: Mapping and mitigating misaligned models</b></i>, Pan et al., <code>2022.01</code> [<a href="https://arxiv.org/abs/2201.03544" target="_blank">Link</a>]</li>
        <li><i><b>Goal misgeneralization in deep reinforcement learning</b></i>, Di Langosco et al., <code>2022.10</code></li>
        <li><i><b>Solving math word problems with process- and outcome-based feedback</b></i>, Uesato et al., <code>2022.11</code> [<a href="https://arxiv.org/abs/2211.14275" target="_blank">Link</a>]</li>
        <li><i><b>Star: Bootstrapping reasoning with reasoning</b></i>, Zelikman et al., <code>2022.11</code></li>
        <li><i><b>Solving math word problems with process- and outcome-based feedback</b></i>, Uesato et al., <code>2022.11</code> [<a href="https://arxiv.org/abs/2211.14275" target="_blank">Link</a>]</li>
        <li><i><b>Let's verify step by step</b></i>, Lightman et al., <code>2023.05</code> [<a href="https://arxiv.org/abs/2305.20050" target="_blank">Link</a>]</li>
        <li><i><b>Critic: Large language models can self-correct with tool-interactive critiquing</b></i>, Gou et al., <code>2023.05</code> [<a href="https://arxiv.org/abs/2305.11738" target="_blank">Link</a>]</li>
        <li><i><b>{LEVER}: Learning to Verify Language-to-Code Generation with Execution</b></i>, Ni et al., <code>2023.07</code> [<a href="https://proceedings.mlr.press/v202/ni23b.html" target="_blank">Link</a>]</li>
        <li><i><b>Reinforced self-training (rest) for language modeling</b></i>, Gulcehre et al., <code>2023.08</code> [<a href="https://arxiv.org/abs/2308.08998" target="_blank">Link</a>]</li>
        <li><i><b>VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search</b></i>, Brandfonbrener et al., <code>2024.02</code> [<a href="https://arxiv.org/abs/2402.08147" target="_blank">Link</a>]</li>
        <li><i><b>Can We Verify Step by Step for Incorrect Answer Detection?</b></i>, Xu et al., <code>2024.02</code> [<a href="https://arxiv.org/abs/2402.10528" target="_blank">Link</a>]</li>
        <li><i><b>Step-dpo: Step-wise preference optimization for long-chain reasoning of llms</b></i>, Lai et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.18629" target="_blank">Link</a>]</li>
        <li><i><b>Llm critics help catch bugs in mathematics: Towards a better mathematical verifier with natural language feedback</b></i>, Gao et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.14024" target="_blank">Link</a>]</li>
        <li><i><b>{LLM} Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</b></i>, Shibo Hao and Yi Gu and Haotian Luo and Tianyang Liu and Xiyan Shao and Xinyuan Wang and Shuhua Xie and Haodi Ma and Adithya Samavedhi and Qiyue Gao and Zhen Wang and Zhiting Hu et al., <code>2024.07</code> [<a href="https://openreview.net/forum?id=b0y6fbSUG0" target="_blank">Link</a>]</li>
        <li><i><b>Enhancing multi-step reasoning abilities of language models through direct q-function optimization</b></i>, Liu et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.09302" target="_blank">Link</a>]</li>
        <li><i><b>Skywork-o1 open series</b></i>, Skywork o1 Team et al., <code>2024.11</code></li>
        <li><i><b>OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning</b></i>, Yu et al., <code>2024.11</code></li>
        <li><i><b>o1-coder: an o1 replication for coding</b></i>, Zhang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.00154" target="_blank">Link</a>]</li>        
        <li><i><b>Math-shepherd: Verify and reinforce llms step-by-step without human annotations</b></i>, Wang et al., <code>2024.12</code></li>
        <li><i><b>Free process rewards without process labels</b></i>, Yuan et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.01981" target="_blank">Link</a>]</li>    
        <li><i><b>AutoPSV: Automated Process-Supervised Verifier</b></i>, Lu et al., <code>2024.12</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/9246aa822579d9b29a140ecdac36ad60-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <li><i><b>Outcome-Refining Process Supervision for Code Generation</b></i>, Yu et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.15118" target="_blank">Link</a>]</li>
        <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
        <li><i><b>Learning to Plan \& Reason for Evaluation with Thinking-LLM-as-a-Judge</b></i>, Saha et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.18099" target="_blank">Link</a>]</li>
        <li><i><b>The lessons of developing process reward models in mathematical reasoning</b></i>, Zhang et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.07301" target="_blank">Link</a>]</li>
        <li><i><b>Error Classification of Large Language Models on Math Word Problems: A Dynamically Adaptive Framework</b></i>, Sun et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.15581" target="_blank">Link</a>]</li>
        <li><i><b>Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models</b></i>, Liu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.09997" target="_blank">Link</a>]</li>
        <li><i><b>Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback</b></i>, Lin et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.10799" target="_blank">Link</a>]</li>
        <li><i><b>STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving</b></i>, Dong et al., <code>2025.01</code></li>
        <li><i><b>STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving</b></i>, Dong et al., <code>2025.01</code></li>
        <li><i><b>Zero-Shot Verification-guided Chain of Thoughts</b></i>, Chowdhury et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.13122" target="_blank">Link</a>]</li>
        <li><i><b>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</b></i>, Xie et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14768" target="_blank">Link</a>]</li>
        <li><i><b>ACECODER: Acing Coder RL via Automated Test-Case Synthesis</b></i>, Zeng et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01718" target="_blank">Link</a>]</li>
        <li><i><b>A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods</b></i>, Puri et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01618" target="_blank">Link</a>]</li>
        <li><i><b>Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges</b></i>, Shrestha et al., <code>2025.02</code> 
        [<a href="https://arxiv.org/abs/2502.08680" target="_blank">Link</a>]</li>
        <li><i><b>Uncertainty-Aware Step-wise Verification with Generative Reward Models</b></i>, Ye et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11250" target="_blank">Link</a>]</li>
        <li><i><b>Uncertainty-Aware Search and Value Models: Mitigating Search Scaling Flaws in LLMs</b></i>, Yu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11155" target="_blank">Link</a>]</li>
        <li><i><b>Theorem Prover as a Judge for Synthetic Data Generation</b></i>, Leang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13137" target="_blank">Link</a>]</li>
        <li><i><b>RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation</b></i>, Zhou et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.09183" target="_blank">Link</a>]</li>
        <li><i><b>Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models</b></i>, Zhou et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.08922" target="_blank">Link</a>]</li>
        <li><i><b>Process Reward Models for LLM Agents: Practical Framework and Directions</b></i>, Choudhury et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.10325" 
        target="_blank">Link</a>]</li>
        <li><i><b>AURORA: Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification</b></i>, Tan et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11520" target="_blank">Link</a>]</li>
        <li><i><b>Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values</b></i>, Zhang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13723" target="_blank">Link</a>]</li>
        <li><i><b>Full-Step-DPO: Self-Supervised Preference Optimization with Step-wise Rewards for Mathematical Reasoning</b></i>, Xu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.14356" target="_blank">Link</a>]</li>
        <li><i><b>QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search</b></i>, Lin et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.02584" target="_blank">Link</a>]</li>
        <li><i><b>Diverse Inference and Verification for Advanced Reasoning</b></i>, Drori et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.09955" target="_blank">Link</a>]</li>
        <li><i><b>QwQ: Reflect Deeply on the Boundaries of the Unknown</b></i>, QwQ Team et al., <code>2025.11</code></li>
        </ul>
        
        <h3>Refinement</h3>
        <ul>
        <li><i><b>Reflection of thought: Inversely eliciting numerical reasoning in language models via solving linear systems</b></i>, Zhou et al., <code>2022.10</code> [<a href="https://arxiv.org/abs/2210.05075" target="_blank">Link</a>]</li>
        <li><i><b>Self-Refine: Iterative Refinement with Self-Feedback</b></i>, Madaan et al., <code>2023.03</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <li><i><b>Reflection-tuning: Data recycling improves llm instruction-tuning</b></i>, Li et al., <code>2023.10</code> [<a href="https://arxiv.org/abs/2310.11716" target="_blank">Link</a>]</li>
        <li><i><b>Reflexion: language agents with verbal reinforcement learning</b></i>, Shinn et al., <code>2023.12</code> [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf" target="_blank">Link</a>]</li>
        <li><i><b>Large Language Models Can Self-Correct with Minimal Effort</b></i>, Zhenyu Wu and Qingkai Zeng and Zhihan Zhang and Zhaoxuan Tan and Chao Shen and Meng Jiang et al., <code>2024.05</code> [<a href="https://openreview.net/forum?id=mmZLMs4l3d" target="_blank">Link</a>]</li>
        <li><i><b>Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b</b></i>, Zhang et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.07394" target="_blank">Link</a>]</li>
        <li><i><b>Large language models have intrinsic self-correction ability</b></i>, Liu et al., <code>2024.06</code> [<a href="https://arxiv.org/abs/2406.15673" target="_blank">Link</a>]</li>
        <li><i><b>Progressive-Hint Prompting Improves Reasoning in Large Language Models</b></i>, Chuanyang Zheng and Zhengying Liu and Enze Xie and Zhenguo Li and Yu Li et al., <code>2024.06</code> [<a href="https://openreview.net/forum?id=UkFEs3ciz8" target="_blank">Link</a>]</li>
        <li><i><b>Toward Adaptive Reasoning in Large Language Models with Thought Rollback</b></i>, Chen et al., <code>2024.07</code> [<a href="https://proceedings.mlr.press/v235/chen24y.html" target="_blank">Link</a>]</li>
        <li><i><b>Mutual reasoning makes smaller llms stronger problem-solvers</b></i>, Qi et al., <code>2024.08</code> [<a href="https://arxiv.org/abs/2408.06195" target="_blank">Link</a>]</li>
        <li><i><b>Re{ST}-{MCTS}*: {LLM} Self-Training via Process Reward Guided Tree Search</b></i>, Dan Zhang and Sining Zhoubian and Ziniu Hu and Yisong Yue and Yuxiao Dong and Jie 
        Tang et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=8rcFOqEud5" target="_blank">Link</a>]</li>
        <li><i><b>Recursive Introspection: Teaching Language Model Agents How to Self-Improve</b></i>, Yuxiao Qu and Tianjun Zhang and Naman Garg and Aviral Kumar et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=DRC9pZwBwR" target="_blank">Link</a>]</li>
        <li><i><b>{LLM} Self-Correction with De{CRIM}: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints</b></i>, Thomas Palmeira Ferraz and Kartik Mehta and Yu-Hsiang Lin and Haw-Shiuan Chang and Shereen Oraby and Sijia Liu and Vivek Subramanian and Tagyoung Chung and Mohit Bansal and Nanyun Peng et al., <code>2024.10</code> [<a href="https://openreview.net/forum?id=RQ6Ff8lso0" target="_blank">Link</a>]</li>
        <li><i><b>O1 Replication Journey: A Strategic Progress Report--Part 1</b></i>, Qin et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.18982" target="_blank">Link</a>]</li>
        <li><i><b>Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up</b></i>, Yuan et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.12323" target="_blank">Link</a>]</li>
        <li><i><b>O1 Replication Journey--Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?</b></i>, Huang et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.16489" target="_blank">Link</a>]</li>
        <li><i><b>Vision-language models can self-improve reasoning via reflection</b></i>, Cheng et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.00855" target="_blank">Link</a>]</li>
        <li><i><b>Confidence vs Critique: A Decomposition of Self-Correction Capability for LLMs</b></i>, Yang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.19513" target="_blank">Link</a>]</li>
        <li><i><b>LLM2: Let Large Language Models Harness System 2 Reasoning</b></i>, Yang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.20372" target="_blank">Link</a>]</li>
        <li><i><b>Understanding the Dark Side of LLMs' Intrinsic Self-Correction</b></i>, Zhang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.14959" target="_blank">Link</a>]</li>
        <li><i><b>Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</b></i>, Wang et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.18585" target="_blank">Link</a>]</li>
        <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
        <li><i><b>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12948" target="_blank">Link</a>]</li>
        <li><i><b>7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient</b></i>, Weihao Zeng and Yuzhen Huang and Wei Liu and Keqing He and Qian Liu and Zejun Ma and Junxian He et al., <code>2025.01</code></li>
        <li><i><b>ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding</b></i>, Sun et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.07861" target="_blank">Link</a>]</li>
        <li><i><b>Critique fine-tuning: Learning to critique is more effective than learning to imitate</b></i>, Wang et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17703" target="_blank">Link</a>]</li>
        <li><i><b>RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques</b></i>, Tang et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.14492" target="_blank">Link</a>]</li>
        <li><i><b>ProgCo: Program Helps Self-Correction of Large Language Models</b></i>, Song et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.01264" target="_blank">Link</a>]</li>
        <li><i><b>URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics</b></i>, Luo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.04686" target="_blank">Link</a>]</li>
        <li><i><b>S$^{2}$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning</b></i>, Ma et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12853" target="_blank">Link</a>]</li>
        <li><i><b>S$^{2}$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning</b></i>, Ma et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12853" target="_blank">Link</a>]</li>
        <li><i><b>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</b></i>, Yang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06772" target="_blank">Link</a>]</li>
        <li><i><b>Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models</b></i>, Yang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.04404" target="_blank">Link</a>]</li>
        <li><i><b>Iterative Deepening Sampling for Large Language Models</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05449" target="_blank">Link</a>]</li>
        <li><i><b>LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!</b></i>, Li et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07374" target="_blank">Link</a>]</li>
        <li><i><b>MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification</b></i>, Sun et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13383" target="_blank">Link</a>]</li>
        </ul>
        <h2>Analysis & Benchmark</h2>
        <h3>Benchmarks</h3>
        <ul>
        <li><i><b>{MR}-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in {LLM}s</b></i>, Zhongshen Zeng and Yinhong Liu and Yingjia Wan and Jingyao Li and Pengguang 
        Chen and Jianbo Dai and Yuxuan Yao and Rongwu Xu and Zehan Qi and Wanru Zhao and Linling Shen and Jianqiao Lu and Haochen Tan and Yukang Chen and Hao Zhang and Zhan Shi and Bailin Wang and Zhijiang Guo and Jiaya Jia et al., <code>2024.06</code> [<a href="https://openreview.net/forum?id=GN2qbxZlni" target="_blank">Link</a>]</li>
        <li><i><b>PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models</b></i>, Song et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.03124" target="_blank">Link</a>]</li>
        <li><i><b>ToolComp: A Multi-Tool Reasoning \& Process Supervision Benchmark</b></i>, Nath et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.01290" target="_blank">Link</a>]</li>
        <li><i><b>MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding</b></i>, Zuo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.18362" target="_blank">Link</a>]</li>
        <li><i><b>ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning</b></i>, Lin et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01100" target="_blank">Link</a>]</li>
        <li><i><b>Evaluating the Systematic Reasoning Abilities of Large Language Models through Graph Coloring</b></i>, Heyman et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07087" target="_blank">Link</a>]</li>
        <li><i><b>Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges</b></i>, Shrestha et al., <code>2025.02</code> 
        [<a href="https://arxiv.org/abs/2502.08680" target="_blank">Link</a>]</li>
        <li><i><b>Evaluating the Systematic Reasoning Abilities of Large Language Models through Graph Coloring</b></i>, Heyman et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07087" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>Explanation for Long CoT</h3>
        <ul>
        <li><i><b>Are Shortest Rationales the Best Explanations for Human Understanding?</b></i>, Shen et al., <code>2022.05</code> [<a href="https://aclanthology.org/2022.acl-short.2/" target="_blank">Link</a>]</li>
        <li><i><b>System 2 Attention (is something you might need too)</b></i>, Weston et al., <code>2023.11</code> [<a href="https://arxiv.org/abs/2311.11829" target="_blank">Link</a>]</li>
        <li><i><b>The Impact of Reasoning Step Length on Large Language Models</b></i>, Jin et al., <code>2024.08</code> [<a href="https://aclanthology.org/2024.findings-acl.108/" target="_blank">Link</a>]</li>
        <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
        <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
        <li><i><b>Compositional Hardness of Code in Large Language Models--A Probabilistic Perspective</b></i>, Wolf et al., <code>2024.09</code> [<a href="https://arxiv.org/abs/2409.18028" target="_blank">Link</a>]</li>
        <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
        <li><i><b>What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective</b></i>, Li et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.23743" target="_blank">Link</a>]</li>
        <li><i><b>When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1</b></i>, McCoy et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.01792" target="_blank">Link</a>]</li>
        <li><i><b>{D}yna{T}hink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models</b></i>, Pan et al., <code>2024.11</code> [<a href="https://aclanthology.org/2024.emnlp-main.814/" target="_blank">Link</a>]</li>
        <li><i><b>Do not think that much for 2+ 3=? on the overthinking of o1-like llms</b></i>, Chen et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.21187" target="_blank">Link</a>]</li>
        <li><i><b>Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models</b></i>, Yuda Song and Hanlin Zhang and Udaya Ghai and Carson Eisenach and Sham M. Kakade and Dean Foster et al., <code>2025.01</code> [<a href="https://openreview.net/forum?id=mtJSMcF3ek" target="_blank">Link</a>]</li>
        <li><i><b>On the reasoning capacity of ai models and how to quantify it</b></i>, Radha et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.13833" target="_blank">Link</a>]</li>
        <li><i><b>Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers?</b></i>, Jin et al., <code>2025.01</code> [<a href="https://aclanthology.org/2025.coling-main.37/" target="_blank">Link</a>]</li>
        <li><i><b>Sft memorizes, rl generalizes: A comparative study of foundation model post-training</b></i>, Chu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17161" target="_blank">Link</a>]</li>
        <li><i><b>Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning</b></i>, Gan et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.15602" target="_blank">Link</a>]</li>
        <li><i><b>Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers</b></i>, Zhang et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.08537" target="_blank">Link</a>]</li>
        <li><i><b>GSM-Infinite: How Do Your LLMs Behave over Infinitely Increasing Context Length and Reasoning Complexity?</b></i>, Zhou et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.05252" target="_blank">Link</a>]</li>
        <li><i><b>When More is Less: Understanding Chain-of-Thought Length in LLMs</b></i>, Wu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07266" target="_blank">Link</a>]</li>
        <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
        <li><i><b>Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts</b></i>, Sadr et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03418" target="_blank">Link</a>]</li>
        <li><i><b>Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention Transformers</b></i>, Amiri et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.02393" target="_blank">Link</a>]</li>
        <li><i><b>Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts</b></i>, Sadr et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03418" target="_blank">Link</a>]</li>
        <li><i><b>The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks</b></i>, Cuadron et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.08235" target="_blank">Link</a>]</li>
        <li><i><b>OverThink: Slowdown Attacks on Reasoning LLMs</b></i>, Kumar et al., <code>2025.02</code></li>
        <li><i><b>When More is Less: Understanding Chain-of-Thought Length in LLMs</b></i>, Wu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07266" target="_blank">Link</a>]</li>
        </ul>
        <h2>Future & Forntier</h2>
        <h3>Agentic \& Embodied Long CoT</h3>
        <ul>
        <li><i><b>Reasoning with language model is planning with world model</b></i>, Hao et al., <code>2023.05</code> [<a href="https://arxiv.org/abs/2305.14992" target="_blank">Link</a>]</li>
        <li><i><b>Reasoning with language model is planning with world model</b></i>, Hao et al., <code>2023.05</code> [<a href="https://arxiv.org/abs/2305.14992" target="_blank">Link</a>]</li>
        <li><i><b>Large language models as commonsense knowledge for large-scale task planning</b></i>, Zhao et al., <code>2023.12</code></li>
        <li><i><b>Tree-Planner: Efficient Close-loop Task Planning with Large Language Models</b></i>, Mengkang Hu and Yao Mu and Xinmiao Chelsey Yu and Mingyu Ding and Shiguang Wu and Wenqi Shao and Qiguang Chen and Bin Wang and Yu Qiao and Ping Luo et al., <code>2024.01</code> [<a href="https://openreview.net/forum?id=Glcsog6zOe" target="_blank">Link</a>]</li>
        <li><i><b>Strategist: Learning Strategic Skills by {LLM}s via Bi-Level Tree Search</b></i>, Jonathan Light and Min Cai and Weiqin Chen and Guanzhi Wang and Xiusi Chen and Wei 
        Cheng and Yisong Yue and Ziniu Hu et al., <code>2024.06</code> [<a href="https://openreview.net/forum?id=UHWBmZuJPF" target="_blank">Link</a>]</li>
        <li><i><b>Strategist: Learning Strategic Skills by {LLM}s via Bi-Level Tree Search</b></i>, Jonathan Light and Min Cai and Weiqin Chen and Guanzhi Wang and Xiusi Chen and Wei 
        Cheng and Yisong Yue and Ziniu Hu et al., <code>2024.06</code> [<a href="https://openreview.net/forum?id=UHWBmZuJPF" target="_blank">Link</a>]</li>
        <li><i><b>Tree search for language model agents</b></i>, Koh et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.01476" target="_blank">Link</a>]</li>
        <li><i><b>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model</b></i>, Hu et al., <code>2024.08</code> [<a href="https://arxiv.org/abs/2408.09559" target="_blank">Link</a>]</li>
        <li><i><b>Agents Thinking Fast and Slow: A Talker-Reasoner Architecture</b></i>, Konstantina Christakopoulou and Shibl Mourad and Maja Mataric et al., <code>2024.10</code> [<a href="https://openreview.net/forum?id=xPhcP6rbI4" target="_blank">Link</a>]</li>
        <li><i><b>Large Language Models for Recommendation with Deliberative User Preference Alignment</b></i>, Fang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.02061" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>Efficient Long CoT</h3>
        <ul>
        <li><i><b>Guiding language model reasoning with planning tokens</b></i>, Wang et al., <code>2023.10</code> [<a href="https://arxiv.org/abs/2310.05707" target="_blank">Link</a>]</li>
        <li><i><b>Synergy-of-thoughts: Eliciting efficient reasoning in hybrid language models</b></i>, Shang et al., <code>2024.02</code> [<a href="https://arxiv.org/abs/2402.02563" 
        target="_blank">Link</a>]</li>
        <li><i><b>Distilling system 2 into system 1</b></i>, Yu et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.06023" target="_blank">Link</a>]</li>
        <li><i><b>Concise thoughts: Impact of output length on llm reasoning and cost</b></i>, Nayab et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.19825" target="_blank">Link</a>]</li>
        <li><i><b>Litesearch: Efficacious tree search for llm</b></i>, Wang et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.00320" target="_blank">Link</a>]</li>    
        <li><i><b>Uncertainty-Guided Optimization on Large Language Model Search Trees</b></i>, Grosse et al., <code>2024.07</code> [<a href="https://arxiv.org/abs/2407.03951" target="_blank">Link</a>]</li>
        <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
        <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
        <li><i><b>Kvsharer: Efficient inference via layer-wise dissimilar KV cache sharing</b></i>, Yang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.18517" target="_blank">Link</a>]</li>
        <li><i><b>Interpretable contrastive monte carlo tree search reasoning</b></i>, Gao et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.01707" target="_blank">Link</a>]</li>
        <li><i><b>Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces</b></i>, Su et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.09918" target="_blank">Link</a>]</li>
        <li><i><b>Vision-language models can self-improve reasoning via reflection</b></i>, Cheng et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.00855" target="_blank">Link</a>]</li>
        <li><i><b>{D}yna{T}hink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models</b></i>, Pan et al., <code>2024.11</code> [<a href="https://aclanthology.org/2024.emnlp-main.814/" target="_blank">Link</a>]</li>
        <li><i><b>Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding</b></i>, Chen et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.04282" target="_blank">Link</a>]</li>
        <li><i><b>{D}yna{T}hink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models</b></i>, Pan et al., <code>2024.11</code> [<a href="https://aclanthology.org/2024.emnlp-main.814/" target="_blank">Link</a>]</li>
        <li><i><b>B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners</b></i>, Zeng et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.17256" target="_blank">Link</a>]</li>
        <li><i><b>Token-budget-aware llm reasoning</b></i>, Han et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.18547" target="_blank">Link</a>]</li>
        <li><i><b>B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners</b></i>, Zeng et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.17256" target="_blank">Link</a>]</li>
        <li><i><b>C3oT: Generating Shorter Chain-of-Thought without Compromising Effectiveness</b></i>, Kang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.11664" target="_blank">Link</a>]</li>
        <li><i><b>Training large language models to reason in a continuous latent space</b></i>, Hao et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.06769" target="_blank">Link</a>]</li>
        <li><i><b>CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models</b></i>, Cheng et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.12932" target="_blank">Link</a>]</li>
        <li><i><b>O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning</b></i>, Luo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12570" target="_blank">Link</a>]</li>
        <li><i><b>URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics</b></i>, Luo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.04686" target="_blank">Link</a>]</li>
        <li><i><b>Reward-Guided Speculative Decoding for Efficient LLM Reasoning</b></i>, Liao et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.19324" target="_blank">Link</a>]</li>
        <li><i><b>Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization</b></i>, Yu et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17974" target="_blank">Link</a>]</li>
        <li><i><b>Efficient Reasoning with Hidden Thinking</b></i>, Shen et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.19201" target="_blank">Link</a>]</li>       
        <li><i><b>On the Query Complexity of Verifier-Assisted Language Generation</b></i>, Botta et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12123" target="_blank">Link</a>]</li>
        <li><i><b>TokenSkip: Controllable Chain-of-Thought Compression in LLMs</b></i>, Xia et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12067" target="_blank">Link</a>]</li>
        <li><i><b>Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation</b></i>, Du et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12492" target="_blank">Link</a>]</li>
        <li><i><b>Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE</b></i>, Huang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06282" target="_blank">Link</a>]</li>
        <li><i><b>On the Convergence Rate of MCTS for the Optimal Value Estimation in Markov Decision Processes</b></i>, Chang et al., <code>2025.02</code></li>
        <li><i><b>CoT-Valve: Length-Compressible Chain-of-Thought Tuning</b></i>, Ma et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.09601" target="_blank">Link</a>]</li>
        <li><i><b>MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification</b></i>, Sun et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.13383" target="_blank">Link</a>]</li>
        <li><i><b>Training Language Models to Reason Efficiently</b></i>, Arora et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.04463" target="_blank">Link</a>]</li><li><i><b>Dynamic Chain-of-Thought: Towards Adaptive Deep Reasoning</b></i>, Wang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.10428" target="_blank">Link</a>]</li>
        <li><i><b>Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking</b></i>, Ziabari et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12470" target="_blank">Link</a>]</li>
        <li><i><b>SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs</b></i>, Xu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12134" target="_blank">Link</a>]</li>
        <li><i><b>Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE</b></i>, Huang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06282" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>Knowledge-Augmented Long CoT</h3>
        <ul>
        <li><i><b>Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation</b></i>, Wang et al., <code>2024.07</code> [<a href="https://proceedings.mlr.press/v235/wang24a.html" target="_blank">Link</a>]</li>
        <li><i><b>Stream of search (sos): Learning to search in language</b></i>, Gandhi et al., <code>2024.07</code></li>
        <li><i><b>CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing</b></i>, Yang et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.16670" target="_blank">Link</a>]</li>
        <li><i><b>Huatuogpt-o1, towards medical complex reasoning with llms</b></i>, Chen et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.18925" target="_blank">Link</a>]</li>
        <li><i><b>RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement</b></i>, Jiang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.12881" target="_blank">Link</a>]</li>
        <li><i><b>Huatuogpt-o1, towards medical complex reasoning with llms</b></i>, Chen et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.18925" target="_blank">Link</a>]</li>
        <li><i><b>O1 Replication Journey--Part 3: Inference-time Scaling for Medical Reasoning</b></i>, Huang et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.06458" 
        target="_blank">Link</a>]</li>
        <li><i><b>MedS $^{3}$: Towards Medical Small Language Models with Self-Evolved Slow Thinking</b></i>, Jiang et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.12051" target="_blank">Link</a>]</li>
        <li><i><b>Search-o1: Agentic search-enhanced large reasoning models</b></i>, Li et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.05366" target="_blank">Link</a>]</li>
        <li><i><b>Chain-of-Retrieval Augmented Generation</b></i>, Wang et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.14342" target="_blank">Link</a>]</li>        
        <li><i><b>Evaluating Large Language Models through Role-Guide and Self-Reflection: A Comparative Study</b></i>, Lili Zhao and Yang Wang and Qi Liu and Mengyun Wang and Wei Chen and Zhichao Sheng and Shijin Wang et al., <code>2025.01</code> [<a href="https://openreview.net/forum?id=E36NHwe7Zc" target="_blank">Link</a>]</li>
        <li><i><b>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model</b></i>, Chen et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03325" target="_blank">Link</a>]</li>
        <li><i><b>Open Deep Research</b></i>, OpenDeepResearch Team et al., <code>2025.02</code></li>
        <li><i><b>DeepRAG: Thinking to Retrieval Step by Step for Large Language Models</b></i>, Guan et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.01142" target="_blank">Link</a>]</li>
        <li><i><b>O1 Embedder: Let Retrievers Think Before Action</b></i>, Yan et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07555" target="_blank">Link</a>]</li> 
        <li><i><b>OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning</b></i>, Lu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.11271" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>Multilingual Long CoT</h3>
        <ul>
        <li><i><b>Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages</b></i>, Qin et al., <code>2023.07</code></li>
        <li><i><b>xcot: Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning</b></i>, Chai et al., <code>2024.01</code> [<a href="https://arxiv.org/abs/2401.07037" target="_blank">Link</a>]</li>
        <li><i><b>Multilingual large language model: A survey of resources, taxonomy and frontiers</b></i>, Qin et al., <code>2024.04</code> [<a href="https://arxiv.org/abs/2404.04925" target="_blank">Link</a>]</li>
        <li><i><b>{A}uto{CAP}: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought</b></i>, Zhang et al., <code>2024.08</code> [<a href="https://aclanthology.org/2024.findings-acl.546/" target="_blank">Link</a>]</li>
        <li><i><b>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</b></i>, Wang et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.17498" target="_blank">Link</a>]</li>
        <li><i><b>A survey of multilingual large language models</b></i>, Qin et al., <code>2025.01</code></li>
        <li><i><b>Demystifying Multilingual Chain-of-Thought in Process Reward Modeling</b></i>, Wang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12663" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>Multimodal Long CoT</h3>
        <ul>
        <li><i><b>Multimodal Chain-of-Thought Reasoning in Language Models</b></i>, Zhuosheng Zhang and Aston Zhang and Mu Li and hai zhao and George Karypis and Alex Smola et al., <code>2024.06</code> [<a href="https://openreview.net/forum?id=y1pPWFVfvR" target="_blank">Link</a>]</li>
        <li><i><b>{M}$^3${C}o{T}: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</b></i>, Chen et al., <code>2024.08</code> [<a href="https://aclanthology.org/2024.acl-long.446/" target="_blank">Link</a>]</li>
        <li><i><b>Visual agents as fast and slow thinkers</b></i>, Sun et al., <code>2024.08</code> [<a href="https://arxiv.org/abs/2408.08862" target="_blank">Link</a>]</li>
        <li><i><b>What factors affect multi-modal in-context learning? an in-depth exploration</b></i>, Qin et al., <code>2024.10</code> [<a href="https://arxiv.org/abs/2410.20482" target="_blank">Link</a>]</li>
        <li><i><b>AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning</b></i>, Xiang et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.11930" target="_blank">Link</a>]</li>
        <li><i><b>AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning</b></i>, Xiang et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.11930" target="_blank">Link</a>]</li>
        <li><i><b>{ARES}: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse {AI} Feedback</b></i>, Byun et al., <code>2024.11</code> [<a href="https://aclanthology.org/2024.emnlp-main.252/" target="_blank">Link</a>]</li>
        <li><i><b>Llava-o1: Let vision language models reason step-by-step</b></i>, Xu et al., <code>2024.11</code> [<a href="https://arxiv.org/abs/2411.10440" target="_blank">Link</a>]</li>
        <li><i><b>Slow Perception: Let's Perceive Geometric Figures Step-by-step</b></i>, Wei et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.20631" target="_blank">Link</a>]</li>
        <li><i><b>Diving into Self-Evolving Training for Multimodal Reasoning</b></i>, Liu et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.17451" target="_blank">Link</a>]</li>
        <li><i><b>Scaling inference-time search with vision value model for improved visual comprehension</b></i>, Xiyao et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.03704" target="_blank">Link</a>]</li>
        <li><i><b>Inference-time scaling for diffusion models beyond scaling denoising steps</b></i>, Ma et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.09732" target="_blank">Link</a>]</li>
        <li><i><b>Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model</b></i>, Ma et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.07246" target="_blank">Link</a>]</li>
        <li><i><b>Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark</b></i>, Hao et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.05444" target="_blank">Link</a>]</li>
        <li><i><b>Virgo: A Preliminary Exploration on Reproducing o1-like MLLM</b></i>, Du et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.01904" target="_blank">Link</a>]</li>
        <li><i><b>Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</b></i>, Li et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.07542" target="_blank">Link</a>]</li>
        <li><i><b>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</b></i>, Guo et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.13926" target="_blank">Link</a>]</li>
        <li><i><b>Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking</b></i>, Wu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.02339" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>Safety</h3>
        <ul>
        <li><i><b>The Impact of Reasoning Step Length on Large Language Models</b></i>, Jin et al., <code>2024.08</code> [<a href="https://aclanthology.org/2024.findings-acl.108/" target="_blank">Link</a>]</li>
        <li><i><b>Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</b></i>, Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che et al., <code>2024.09</code> [<a href="https://openreview.net/forum?id=pC44UMwy2v" target="_blank">Link</a>]</li>
        <li><i><b>Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits</b></i>, Li et al., <code>2024.12</code> [<a href="https://arxiv.org/abs/2412.12510" target="_blank">Link</a>]</li>
        <li><i><b>o3-mini vs DeepSeek-R1: Which One is Safer?</b></i>, Arrieta et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.18438" target="_blank">Link</a>]</li> 
        <li><i><b>Efficient Reasoning with Hidden Thinking</b></i>, Shen et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.19201" target="_blank">Link</a>]</li>       
        <li><i><b>Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking</b></i>, Cheng et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.01306" target="_blank">Link</a>]</li>
        <li><i><b>Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection</b></i>, Zhao et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.02295" target="_blank">Link</a>]</li>
        <li><i><b>Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies</b></i>, Parmar et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17030" target="_blank">Link</a>]</li>
        <li><i><b>Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation</b></i>, Arrieta et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17749" target="_blank">Link</a>]</li>
        <li><i><b>International AI Safety Report</b></i>, Bengio et al., <code>2025.01</code> [<a href="https://arxiv.org/abs/2501.17805" target="_blank">Link</a>]</li>
        <li><i><b>OverThink: Slowdown Attacks on Reasoning LLMs</b></i>, Kumar et al., <code>2025.02</code></li>
        <li><i><b>MetaSC: Test-Time Safety Specification Optimization for Language Models</b></i>, Gallego et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.07985" target="_blank">Link</a>]</li>
        <li><i><b>Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment</b></i>, Wang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.04040" target="_blank">Link</a>]</li>
        <li><i><b>H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking</b></i>, Kuo et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12893" target="_blank">Link</a>]</li>
        <li><i><b>The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1</b></i>, Zhou et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12659" target="_blank">Link</a>]</li>
        <li><i><b>Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models</b></i>, Lu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12825" target="_blank">Link</a>]</li>
        <li><i><b>SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities</b></i>, Jiang et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12025" target="_blank">Link</a>]</li>
        <li><i><b>BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack</b></i>, Zhu et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.12202" target="_blank">Link</a>]</li>
        <li><i><b>Emergent Response Planning in LLM</b></i>, Dong et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.06258" target="_blank">Link</a>]</li>
        </ul>
        
        <h3>proper reward design</h3>
        <ul>
        <li><i><b>Demystifying Long Chain-of-Thought Reasoning in LLMs</b></i>, Yeo et al., <code>2025.02</code> [<a href="https://arxiv.org/abs/2502.03373" target="_blank">Link</a>]</li>
        </ul>  
      </div>
      </ul>
    </div>
    <div class="columns is-centered">
      <div class="column is-9">
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
